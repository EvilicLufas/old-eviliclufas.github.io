<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Mainly record the thoughts &amp; techniques in shool learning &amp; rearch in AI">
<meta property="og:type" content="website">
<meta property="og:title" content="Vellichor Kyst">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Vellichor Kyst">
<meta property="og:description" content="Mainly record the thoughts &amp; techniques in shool learning &amp; rearch in AI">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Vellichor Kyst</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Vellichor Kyst</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"> </p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/17/%E9%AB%98%E5%85%89%E8%B0%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vellichor">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vellichor Kyst">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/17/%E9%AB%98%E5%85%89%E8%B0%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F/" itemprop="url">高光谱数据集格式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-17T00:23:55+08:00">
                2019-12-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Indian-pines数据集"><a href="#Indian-pines数据集" class="headerlink" title="Indian_pines数据集"></a>Indian_pines数据集</h1><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>Indian Pines数据集包括<strong>200个光谱波段</strong>(24个光谱波段由于噪音消除，留下104-108、150-162)，波长范围为0.4 -2.5 μm。空间分辨率 为 20米/像素。其分辨率为 <strong>145 × 145</strong> 像素。该数据集包含<strong>16个</strong>具有代表性的土地覆盖类别。</p>
<h2 id="Indian-pines-gt-mat文件"><a href="#Indian-pines-gt-mat文件" class="headerlink" title="Indian_pines_gt.mat文件"></a>Indian_pines_gt.mat文件</h2><p>(x,y,l):x表示横坐标，取值范围0-144；y表示纵坐标，取值范围0-144；l表示label，取值范围0-16，0表示背景，1-16表示class。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'Indian_pines_gt.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line"><span class="comment"># print(data.keys())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"indian_pines_gt"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint8</span><br><span class="line">数组元素总数： 21025</span><br><span class="line">数组形状： (145, 145)</span><br><span class="line">数组的维度数目 2</span><br><span class="line">[ 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  0  3  3  3</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15</span><br><span class="line"> 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15  0</span><br><span class="line">  0 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0]</span><br></pre></td></tr></table></figure>



<h2 id="indiancmap-mat文件"><a href="#indiancmap-mat文件" class="headerlink" title="indiancmap.mat文件"></a>indiancmap.mat文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取indiancmap.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'indiancmap.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"mycmap"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;mycmap&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： float64</span><br><span class="line">数组元素总数： 192</span><br><span class="line">数组形状： (64, 3)</span><br><span class="line">数组的维度数目 2</span><br><span class="line">[1. 1. 1.]</span><br></pre></td></tr></table></figure>

<h2 id="Indian-pines-corrected-mat文件"><a href="#Indian-pines-corrected-mat文件" class="headerlink" title="Indian_pines_corrected.mat文件"></a>Indian_pines_corrected.mat文件</h2><p>(x,y,f):x表示横坐标，取值范围0-144；y表示纵坐标，取值范围0-144；f表示光谱波段，是200维的向量，表示各波段的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'Indian_pines_corrected.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line"><span class="comment"># print(data.keys())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"indian_pines_corrected"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint16</span><br><span class="line">数组元素总数： 4205000</span><br><span class="line">数组形状： (145, 145, 200)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[3172 4142 4506 ... 1057 1020 1020]</span><br><span class="line"> [2580 4266 4502 ... 1064 1029 1020]</span><br><span class="line"> [3687 4266 4421 ... 1061 1030 1016]</span><br><span class="line"> ...</span><br><span class="line"> [2570 3890 4320 ... 1042 1021 1015]</span><br><span class="line"> [3170 4130 4320 ... 1054 1024 1020]</span><br><span class="line"> [3172 3890 4316 ... 1043 1034 1016]]</span><br></pre></td></tr></table></figure>

<h1 id="KSC数据集"><a href="#KSC数据集" class="headerlink" title="KSC数据集"></a>KSC数据集</h1><h2 id="数据集介绍-1"><a href="#数据集介绍-1" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>KSC[38]是来自于AVIRIS传感器在佛罗里达的区域图像。光谱覆盖范围从0.4到2.5μm。图像包含224个波段，614×512像素。去除吸水率和低信噪比波段后，图像包含176个波段，空间分辨率为18米。<u>本文选取5211个标记像素的13个类作为实验代表。由于某些植被类型的光谱特征相似，导致该环境的土地覆盖识别困难，因此选择图像数据。</u></p>
<h2 id="KSC-mat文件"><a href="#KSC-mat文件" class="headerlink" title="KSC.mat文件"></a>KSC.mat文件</h2><p>(x,y,f):x表示横坐标，取值范围0-511；y表示纵坐标，取值范围0-613；f表示光谱波段，是176维的向量，表示各波段的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'KSC.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"KSC"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;KSC&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint16</span><br><span class="line">数组元素总数： 55328768</span><br><span class="line">数组形状： (512, 614, 176)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[11 26 19 ... 63 64 58]</span><br><span class="line"> [ 4 22 10 ... 38 44 43]</span><br><span class="line"> [ 6 16  9 ... 48 47 49]</span><br><span class="line"> ...</span><br><span class="line"> [28 44 32 ... 38 56 46]</span><br><span class="line"> [28 37 31 ... 39 42 31]</span><br><span class="line"> [17 37 27 ... 35 39 34]]</span><br></pre></td></tr></table></figure>

<h2 id="KSC-gt-mat"><a href="#KSC-gt-mat" class="headerlink" title="KSC_gt.mat"></a>KSC_gt.mat</h2><p>(x,y,f):x表示横坐标，取值范围0-511；y表示纵坐标，取值范围0-613；f表示光谱波段，是176维的向量，表示各波段的label值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'KSC_gt.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"KSC_gt"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">336</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;KSC_gt&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint8</span><br><span class="line">数组元素总数： 314368</span><br><span class="line">数组形状： (512, 614)</span><br><span class="line">数组的维度数目 2</span><br><span class="line">[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0 10 10 10 10 10 10 10 10 10  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  2  2  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0]</span><br></pre></td></tr></table></figure>

<h1 id="University-of-Pavia数据集"><a href="#University-of-Pavia数据集" class="headerlink" title="University of Pavia数据集"></a>University of Pavia数据集</h1><h2 id="数据集介绍-2"><a href="#数据集介绍-2" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><h2 id="Pavia-mat文件"><a href="#Pavia-mat文件" class="headerlink" title="Pavia.mat文件"></a>Pavia.mat文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'Pavia.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"pavia"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">336</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">D:\Anaconda3\envs\py36\python.exe E:/PycharmProjects/UNet-Satellite-Image-Segmentation/MatTest/__init__.py</span><br><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;pavia&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint16</span><br><span class="line">数组元素总数： 79931280</span><br><span class="line">数组形状： (1096, 715, 102)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[ 960  512  403 ... 2677 2675 2717]</span><br><span class="line"> [ 379  401  648 ... 2622 2586 2578]</span><br><span class="line"> [1022  803  806 ... 2163 2135 2136]</span><br><span class="line"> ...</span><br><span class="line"> [ 146  183  377 ...   23   14   11]</span><br><span class="line"> [ 529  448  306 ...   48   48   41]</span><br><span class="line"> [ 281  453  414 ...   53   54   22]]</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>

<h2 id="Pavia-gt-mat文件"><a href="#Pavia-gt-mat文件" class="headerlink" title="Pavia_gt.mat文件"></a>Pavia_gt.mat文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'Pavia_gt.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"pavia_gt"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">336</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;pavia_gt&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint8</span><br><span class="line">数组元素总数： 783640</span><br><span class="line">数组形状： (1096, 715)</span><br><span class="line">数组的维度数目 2</span><br><span class="line">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 9 9 9 0 0 0 0 0 0 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 8</span><br><span class="line"> 8 0 0 8 8 8 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 8 8 8 8 8 8 8 8 8</span><br><span class="line"> 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0</span><br><span class="line"> 0 0 0 0 0 0 8 8 8 8 8 8 8 8 8 8 8 8 8 0 0 0 0 0 0 8 8 8 8 8 8 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 9 9</span><br><span class="line"> 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 9 9</span><br><span class="line"> 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 6 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 6 6 6 6 6 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 0 0 0 0 0 5 5 5 5 5 5 5 5 5 5 5 5 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 5 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1</span><br><span class="line"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span><br><span class="line"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span><br><span class="line"> 1 1 1 1 1 1 1 1 1 1 1 1]</span><br></pre></td></tr></table></figure>

<h2 id="数据集介绍-3"><a href="#数据集介绍-3" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>University of Pavia数据集包括103个光谱波段(12个光谱波段由于噪音消除)，波长范围为0.43 - 0.86 μm。空间分辨率 为 1.3 m/像素。图像尺寸为610×340像素，光谱分辨率为10nm，该数据集包含9个具有代表性的土地覆盖类别</p>
<h2 id="PaviaU-mat文件"><a href="#PaviaU-mat文件" class="headerlink" title="PaviaU.mat文件"></a>PaviaU.mat文件</h2><p>(x,y,f):x表示横坐标，取值范围0-609；y表示纵坐标，取值范围0-339；f表示光谱波段，是103维的向量，表示各波段的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'PaviaU.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"paviaU"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">336</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;paviaU&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint16</span><br><span class="line">数组元素总数： 21362200</span><br><span class="line">数组形状： (610, 340, 103)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[ 879  871  788 ...  239  226  221]</span><br><span class="line"> [ 747  561  409 ...  314  302  292]</span><br><span class="line"> [ 536  369  380 ...  318  308  315]</span><br><span class="line"> ...</span><br><span class="line"> [ 502  575  491 ... 2552 2520 2531]</span><br><span class="line"> [ 498  439  233 ... 2160 2137 2113]</span><br><span class="line"> [ 594  424  450 ... 2715 2774 2810]]</span><br></pre></td></tr></table></figure>

<h2 id="PaviaU-gt-mat文件"><a href="#PaviaU-gt-mat文件" class="headerlink" title="PaviaU_gt.mat文件"></a>PaviaU_gt.mat文件</h2><p>(x,y,l):x表示横坐标，取值范围0-609；y表示纵坐标，取值范围0-339；l表示该位置对应的label值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'PaviaU_gt.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"paviaU_gt"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">336</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;paviaU_gt&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint8</span><br><span class="line">数组元素总数： 207400</span><br><span class="line">数组形状： (610, 340)</span><br><span class="line">数组的维度数目 2</span><br><span class="line">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3</span><br><span class="line"> 0 0 0 4 0 0 0 0 0 0 0 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 4 4 4 4 4 4 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 7 7 7 7 7 7 7 7 7 7 7 7 0 0 0 0 0 0 9 9 9 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6</span><br><span class="line"> 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6</span><br><span class="line"> 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure>

<h1 id="Salinas数据集"><a href="#Salinas数据集" class="headerlink" title="Salinas数据集"></a>Salinas数据集</h1><h2 id="数据集介绍-4"><a href="#数据集介绍-4" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>Salinas数据集包含204个光谱波段(20个光谱波段由于噪音消除)。图像尺寸为 512 × 217像素，空间分辨率为3.7米/像素，光谱分辨率为10nm。该数据集包含16个代表性类别。</p>
<h2 id="Salinas-mat文件"><a href="#Salinas-mat文件" class="headerlink" title="Salinas.mat文件"></a>Salinas.mat文件</h2><p>原始文件，没有去除噪音光谱波段。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取Indian_pines_gt.mat文件</span></span><br><span class="line">data = scipy.io.loadmat(<span class="string">'Salinas.mat'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看mat文件中的所有变量</span></span><br><span class="line">print(data.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为数组</span></span><br><span class="line">X_train =  np.array(data[<span class="string">"salinas"</span>])</span><br><span class="line"><span class="comment"># print(X_train)</span></span><br><span class="line">print(<span class="string">"数据类型"</span>,type(X_train))           <span class="comment">#打印数组数据类型</span></span><br><span class="line">print(<span class="string">"数组元素数据类型："</span>,X_train.dtype) <span class="comment">#打印数组元素数据类型</span></span><br><span class="line">print(<span class="string">"数组元素总数："</span>,X_train.size)      <span class="comment">#打印数组尺寸，即数组元素总数</span></span><br><span class="line">print(<span class="string">"数组形状："</span>,X_train.shape)         <span class="comment">#打印数组形状</span></span><br><span class="line">print(<span class="string">"数组的维度数目"</span>,X_train.ndim)      <span class="comment">#打印数组的维度数目</span></span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">336</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;salinas&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： int16</span><br><span class="line">数组元素总数： 24887296</span><br><span class="line">数组形状： (512, 217, 224)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[378 476 734 ...  16  27  10]</span><br><span class="line"> [378 392 669 ...  22  33  21]</span><br><span class="line"> [378 476 603 ...  11  23  12]</span><br><span class="line"> ...</span><br><span class="line"> [363 470 734 ...  17  21  10]</span><br><span class="line"> [363 470 669 ...  19  31  18]</span><br><span class="line"> [363 387 734 ...  17  27  14]]</span><br></pre></td></tr></table></figure>

<h2 id="Salinas-gt-mat文件"><a href="#Salinas-gt-mat文件" class="headerlink" title="Salinas_gt.mat文件"></a>Salinas_gt.mat文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;salinas_gt&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint8</span><br><span class="line">数组元素总数： 111104</span><br><span class="line">数组形状： (512, 217)</span><br><span class="line">数组的维度数目 2</span><br><span class="line">[13 13 13 13 13 13 13 13 13 13 13  0  0  0  0  0  0 14 14 14 14 14 14 14</span><br><span class="line"> 14 14 14 14 14 14 14 14 14 14  0  0  0  0  0  0  0 10 10 10 10 10 10 10</span><br><span class="line"> 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10</span><br><span class="line"> 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span><br><span class="line">  0]</span><br></pre></td></tr></table></figure>

<h2 id="Salinas-corrected-mat文件"><a href="#Salinas-corrected-mat文件" class="headerlink" title="Salinas_corrected.mat文件"></a>Salinas_corrected.mat文件</h2><p>原始图像224个光谱波段，20个光谱波段由于噪音消除，于是最终实验使用的是204个光谱波段。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;salinas_corrected&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： int16</span><br><span class="line">数组元素总数： 22665216</span><br><span class="line">数组形状： (512, 217, 204)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[378 476 734 ...  44  16  27]</span><br><span class="line"> [378 392 669 ...  48  22  33]</span><br><span class="line"> [378 476 603 ...  50  11  23]</span><br><span class="line"> ...</span><br><span class="line"> [363 470 734 ...  42  17  21]</span><br><span class="line"> [363 470 669 ...  48  19  31]</span><br><span class="line"> [363 387 734 ...  44  17  27]]</span><br></pre></td></tr></table></figure>

<h2 id="数据集介绍-5"><a href="#数据集介绍-5" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>感觉SalinasA数据集可能是验证集。</p>
<h2 id="SalinasA-mat文件"><a href="#SalinasA-mat文件" class="headerlink" title="SalinasA.mat文件"></a>SalinasA.mat文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;salinasA&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： int16</span><br><span class="line">数组元素总数： 1598912</span><br><span class="line">数组形状： (83, 86, 224)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[435 389 734 ...  25  43  20]</span><br><span class="line"> [364 473 734 ...  21  31  14]</span><br><span class="line"> [435 557 734 ...  17  27  24]</span><br><span class="line"> ...</span><br><span class="line"> [364 473 734 ...  19  29  16]</span><br><span class="line"> [435 473 734 ...  15  25  12]</span><br><span class="line"> [441 400 671 ...     17  10]]</span><br></pre></td></tr></table></figure>

<h2 id="SalinasA-gt-mat文件"><a href="#SalinasA-gt-mat文件" class="headerlink" title="SalinasA_gt.mat文件"></a>SalinasA_gt.mat文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;salinasA_gt&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： uint8</span><br><span class="line">数组元素总数： 7138</span><br><span class="line">数组形状： (83, 86)</span><br><span class="line">数组的维度数目 2</span><br><span class="line">[ 0  0  0  0  0 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11  0</span><br><span class="line">  0  0  0  0  0  0 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12</span><br><span class="line"> 12 12 12 12 12 12 12 12 12 12 12 12 12  0  0  0  0  0  0  0  0 13 13 13</span><br><span class="line"> 13 13 13 13 13 13 13 13 13 13 13  0  0  0]</span><br></pre></td></tr></table></figure>



<h2 id="SalinasA-corrected-mat文件"><a href="#SalinasA-corrected-mat文件" class="headerlink" title="SalinasA_corrected.mat文件"></a>SalinasA_corrected.mat文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;salinasA_corrected&apos;])</span><br><span class="line">数据类型 &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">数组元素数据类型： int16</span><br><span class="line">数组元素总数： 1456152</span><br><span class="line">数组形状： (83, 86, 204)</span><br><span class="line">数组的维度数目 3</span><br><span class="line">[[435 389 734 ...  68  25  43]</span><br><span class="line"> [364 473 734 ...  50  21  31]</span><br><span class="line"> [435 557 734 ...  48  17  27]</span><br><span class="line"> ...</span><br><span class="line"> [364 473 734 ...  44  19  29]</span><br><span class="line"> [435 473 734 ...  38  15  25]</span><br><span class="line"> [441 400 671 ...  30   9  17]]</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/17/End-to-End%20Machine%20Learning%20with%20TensorFlow%20on%20GCP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vellichor">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vellichor Kyst">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/17/End-to-End%20Machine%20Learning%20with%20TensorFlow%20on%20GCP/" itemprop="url">End-to-End Machine Learning with TensorFlow on GCP</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-17T00:22:47+08:00">
                2019-12-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="End-to-End-Machine-Learning-with-TensorFlow-on-GCP"><a href="#End-to-End-Machine-Learning-with-TensorFlow-on-GCP" class="headerlink" title="End-to-End Machine Learning with TensorFlow on GCP"></a><a href="https://www.coursera.org/learn/end-to-end-ml-tensorflow-gcp/home/welcome" target="_blank" rel="noopener">End-to-End Machine Learning with TensorFlow on GCP</a></h1><h2 id="第-1-周"><a href="#第-1-周" class="headerlink" title="第 1 周"></a><a href="https://www.coursera.org/learn/end-to-end-ml-tensorflow-gcp/home/week/1" target="_blank" rel="noopener">第 1 周</a></h2><h3 id="1-Fully-managed-ML"><a href="#1-Fully-managed-ML" class="headerlink" title="1. Fully managed ML"></a>1. Fully managed ML</h3><blockquote>
<p>Let’s do a fast review of the steps involved when doing machine learning on GCP. These are the steps involved in any machine learning project, but we’ll focus on doing them with Google Cloud Platform Tools. Like most software libraries, TensorFlow contains multiple abstraction levels, tf layers, tf losses et cetera. These are high level representations of useful neural network components. These modules provide components that are useful when building custom neural network models. You often don’t need a custom ML model, the estimator API is a high level API. It knows how to do distributed training, it knows how to evaluate, how to create a checkpoint, how to save a model, how to set it up for TensorFlow Serving. It comes with everything done in a sensible way that fits most ML models in production. In this course, we will work with TensorFlow at the tf estimator level of abstraction. Cloud ML Engine is orthogonal to this hierarchy. Regardless of which abstraction level you’re writing your code at, CMLE gives you a managed service for training and deploying TensorFlow models. </p>
</blockquote>
<p>让我们快速回顾一下在 GCP 上进行机器学习所涉及的步骤。这些是任何机器学习项目都会涉及到的步骤，但我们将重点介绍如何使用 Google 云平台工具来完成这些步骤。像大多数软件库一样，TensorFlow 包含多个抽象级别，tf 层，tf 丢失等等。这些都是有用的神经网络组件的高级表示。这些模块提供的组件在构建定制神经网络模型时非常有用。你通常不需要一个定制的机器学习模型，估计器 API 是一个高级的 API。它知道如何进行分布式培训，它知道如何评估，如何创建检查点，如何保存模型，如何为 TensorFlow Serving 设置模型。它以一种适合大多数机器学习生产模式的合理方式完成了所有的事情。在本课程中，我们将在抽象的 tf 估计级别上使用 TensorFlow。云机器学习引擎与这个层次结构是正交的。无论您在哪个抽象层编写代码，CMLE 都为您提供了一个用于培训和部署 TensorFlow 模型的托管服务。</p>
<blockquote>
<p>If you have data that fits in memory, pretty much any machine learning framework will work. Once your data sets get larger, your data will not fit into memory and you need a more sophisticated performant ML framework. This is where TensorFlow comes in. You can use TensorFlow estimators not just for deep learning but also for things like Boosted Regression Trees. But as we discussed in the first specialization, there are ways to architect deep neural networks so that they get the benefits of bagging and boosting, and so we will simply focus on one technology, Neural Networks. In real-world problems, there’s often very little benefit to a different machine learning algorithm. You should spend any time and resources you have on getting better data. But as we said, many machine learning frameworks can handle toy problems. But what are some of the important things to think about when it comes to building effective machine learning models? The first and most important thing is that you need to figure out a way to train the model on as much data as you can. Don’t sample the data, don’t aggregate the data, use as much data as you can. As your data size increases, batching and distribution become extremely important. You will need to take your data and split it into batches, and then you need to train, but then you need to also distribute your training over many machines. This is not as simple as MapReduce, where things are embarrassingly parallel. Things like gradient descent optimizations are not embarrassingly parallel. You will need parameter servers that form a shared memory that’s updated during each epoch. Sometimes people think they can take a shortcut in order to keep the training simple by getting a bigger and bigger machine with lots of GPUs. They often regret that decision because at some point, you will hit the limit of whatever single machine you’re using. Scaling out is the answer, not scaling up. </p>
</blockquote>
<p>如果你有适合内存的数据，几乎任何机器学习框架都可以工作。一旦你的数据集变得更大，你的数据将无法放入内存，你需要一个更复杂的性能更高的机器学习框架。这就是 TensorFlow 的用武之地。你不仅可以使用 TensorFlow 估计器来进行深度学习，还可以使用诸如升级回归树之类的东西。但是正如我们在第一个专题中所讨论的，有一些方法可以构建深层神经网络，这样它们就可以得到装袋和增强的好处，所以我们将简单地关注一种技术，神经网络。在真实世界的问题中，不同的机器学习算法通常没有什么好处。你应该花费你所有的时间和资源来获得更好的数据。但正如我们所说的，许多机器学习框架可以处理玩具问题。但是，在建立有效的机器学习模型时，需要考虑哪些重要因素呢？首先，也是最重要的一点是，您需要找到一种方法，以尽可能多的数据来训练模型。不要采样数据，不要聚合数据，尽可能多地使用数据。随着数据大小的增加，批处理和分发变得极其重要。您需要将数据分成批，然后需要进行培训，但是还需要将培训分布到许多机器上。这可不像 MapReduce 那么简单，因为它们的并行性令人尴尬。类似于梯度下降法优化的事情并不是令人尴尬的并行的。您将需要形成共享内存的参数服务器，该内存在每个历元期间更新。有时候人们认为他们可以走捷径，以保持训练的简单，得到一个越来越大的机器与大量的 gpu。他们经常后悔这个决定，因为在某个时刻，你将会达到你所使用的任何一台机器的极限。扩大规模是答案，而不是扩大规模。</p>
<blockquote>
<p>Another common shortcut that people take is to sample their data, so that it’s small enough to do machine learning on the hardware they happened to have. They’re leaving substantial performance gains on the table if they do that. Using all the available data and devising a plan to collect even more data is often the difference between ML that doesn’t work and machine learning that appears magical. It’s rare that you can build an effective machine learning model from just the raw data. Instead, you have to do feature engineering to get great machine learning models. So, the second thing you need to build effective machine learning is that you need feature engineering. Many of the major improvements to machine learning happen when human insights come into the problem. In machine learning, you bring human insights, what your experts know about the problem in the form of new features. You will need to pre-process the data, you will need to scale the data, encode it et cetera, and you need to create new features, and you need to do these two things on the large dataset, and it needs to be distributed, and it needs to be done on the cloud. The third thing that you need for effective machine learning is to use an appropriate model architecture. Different types of problems are better addressed with different types of models. For example, if you have a text classification problem, you want to be able to use CNNs and RNNs, things that we will look at in this specialization. This is where TensorFlow comes in. TensorFlow is the number one machine learning software repository. We, Google that is, we open-sourced TensorFlow because it can enable so many other companies to build great machine learning models. TensorFlow is highly performant. You can train models on CPUs, GPUs, TPUs, et cetera. Another advantage, you’re also not locked in when you work with Cloud ML on GCP because the code that you write, TensorFlow, is based on open-source. So, why use TensorFlow? Because it can work with big data, it can capture many types of feature transformations, and it has implementations of many kinds of model architectures.</p>
</blockquote>
<p>人们采取的另一个常见的快捷方式是对数据进行采样，这样它就足够小，可以在他们碰巧拥有的硬件上进行机器学习。如果他们这样做的话，他们就会留下巨大的业绩收益。利用所有可用的数据，并设计一个计划来收集更多的数据，这往往是机器学习和机器学习之间的区别，机器学习似乎是神奇的。仅仅从原始数据就能建立一个有效的机器学习模型是非常罕见的。相反，你必须做功能工程，以获得伟大的机器学习模型。所以，构建有效的机器学习的第二件事是你需要特性工程。许多机器学习的重大改进都发生在人类洞察力出现问题的时候。在机器学习中，你带来了人类的洞察力，你的专家以新特性的形式知道问题的所在。你需要对数据进行预处理，你需要对数据进行扩展，编码等等，你需要创建新的特性，你需要在大数据集上做这两件事，它需要被分布，它需要在云上完成。有效的机器学习需要的第三件事是使用合适的模型架构。不同类型的问题最好用不同类型的模型来解决。例如，如果您有一个文本分类问题，您希望能够使用 cnn 和 rnn，我们将在这个专门化中看到的东西。这就是 TensorFlow 的用武之地。是排名第一的机器学习软件资源库。我们，也就是谷歌，我们开源了 TensorFlow，因为它可以让许多其他公司建立伟大的机器学习模型。Tensorflow 是高性能的。您可以在 cpu、 gpu、 TPUs 等上训练模型。另一个优点是，在 GCP 上使用 Cloud ML 时，您也不会被锁定，因为您编写的代码 TensorFlow 是基于开放源码的。那么，为什么要使用 TensorFlow 呢？因为它可以处理大数据，它可以捕获许多类型的特性转换，并且它有许多种模型架构的实现。</p>
<h3 id="2-Exploring-the-dataset"><a href="#2-Exploring-the-dataset" class="headerlink" title="2.  Exploring the dataset"></a>2.  Exploring the dataset</h3><blockquote>
<p>The first lab is about exploring the data. Why are we exploring the data? Why don’t we just take all the columns in the dataset and feed them into the machine learning model? Shouldn’t the machine learning model be able to figure out that some of the columns aren’t needed? Maybe give them zero weight? Isn’t the point of the machine learning model to learn how to combine the columns so as to get the label that we want? Well, real life doesn’t work that way. Many times that data, as recorded, isn’t what you expect. Show me a dataset that no one is actively visualizing, whether in the form of dashboards or charts or something, and I’m quite confident that much of the data will be missing or even wrong. In the real world, there are surprisingly many intricacies hidden in the data, and if we use the data without developing an understanding of it, we will end up using the data in a way that will make productionization very hard. The thing to remember about productionization is that during production, you’re going to have to deal with the data as it comes in. So, it’ll make productionization very hard and we’ll see a few examples of this. You are probably doing this specialization because you saw images, sequences, recommendation models, all listed in the set of courses. However, all five courses in the first specialization were all on structured data.</p>
</blockquote>
<p>第一个实验室是关于探索数据的。我们为什么要探索这些数据？为什么我们不把数据集中的所有列输入到机器学习模型中呢？难道机器学习模型不应该能够指出一些列是不需要的吗？也许给他们零weight？难道机器学习模型的重点不是学习如何组合列以获得我们想要的标签吗？现实生活不是这样的。很多时候，这些数据，根据记录，并不是你所期望的。给我看一个没有人积极可视化的数据集，不管是仪表盘还是图表或其他什么，我相信很多数据将会丢失，甚至是错误的。在现实世界中，隐藏在数据中的错综复杂令人惊讶，如果我们使用这些数据而不去理解它们，我们最终会以一种使生产变得非常困难的方式来使用这些数据。关于生产化，需要记住的是，在生产过程中，你必须在数据到来的时候处理它。所以，这会使生产变得非常困难，我们会看到这方面的一些例子。您之所以这样做，可能是因为您看到了图像、序列、推荐模型，它们都列在课程集中。然而，第一个专业化的所有五门课程都是关于结构化数据的。</p>
<blockquote>
<p>Why? Even though image models and text models get all the press, even at Google, most of our machine learning models operate on structured data. That’s what this table shows. MLP is multilayer perceptron, your traditional feedforward fully connected neural network with four or five layers, and that’s what you tend to use for structured data. Nearly two thirds of our models are MLPs. LSTM, long short-term memory models, are what you tend to use on text and time series models. That’s 29% of all of our models. CNNs, convolutional neural networks, these are the models you tend to use primarily for image models. Although you can also successfully use them for tasks like text classification. CNNs are just five percent of models. This explains why we have focused so much on structured data models. These are, quite simply, the most common types of models that you will encounter in practice. Our goal is to predict the weight of newborns so that all newborns can get the care that they need. This scenario is this, a mother calls a clinic and says that she’s on her way. At that point, the nurse uses our application to predict what the weight of the newborn baby is going to be, and if the weight is below some number, the nurse arranges for special facilities like incubators, maybe different types of doctors et cetera, and this is so that we can get babies the care that they need. So, this is the application that we will build. Essentially, the nurse puts in the mother’s age, the gestation weeks assuming that the baby is born today, how many babies - single, twins et cetera, and the baby’s gender if it is known. The nurse hits “Predict”, the ML model runs, and the nurse gets back a prediction of 7.19 pounds or 4.36 pounds, depending on the inputs, and then the nurse arranges for special facilities for the babies on the right, and that’s the way it works. So, this is what we will build. For machine learning, we need training data. In our case, the US government has been collecting statistics on births for many years. That data is available as a sample dataset in BigQuery. It’s reasonably sized, it has about 140 million rows, 22 gigs of data. We can use this dataset to build a machine learning model. In reality, of course, you don’t want to use data this old, 1969 to 2008, but let’s ignore the fact that the sample dataset stops in 2008 because this is a learning opportunity.</p>
</blockquote>
<p>为什么？即使图像模型和文本模型得到了所有的媒体，即使在谷歌，我们的大多数机器学习模型操作的结构化数据。这就是这个表格所显示的。多层前向神经网络是一种多层感知机神经网络，传统的前向完全连接的神经网络，有4到5层，这就是结构化数据使用的方式。我们的车型中近三分之二是 mlp。Lstm，长期短期记忆模型，是你倾向于在文本和时间序列模型中使用的。这是我们所有模型的29% 。Cnns，卷积神经网络，这些是你倾向于主要用于图像模型的模型。尽管您也可以成功地将它们用于文本分类等任务。Cnn 只是模型的百分之五。这就解释了为什么我们如此关注结构化数据模型。这些都是您在实践中将遇到的最常见的模型类型。我们的目标是预测新生儿的体重，这样所有的新生儿都能得到他们所需要的护理。这个场景是这样的，一位母亲打电话给诊所，说她在路上了。这时，护士使用我们的应用程序来预测新生儿的体重，如果体重低于某个数字，护士就会安排一些特殊的设施，比如恒温箱，也许还有不同类型的医生等等，这样我们就可以给婴儿提供他们所需要的护理。这就是我们将要构建的应用程序。基本上，护士把母亲的年龄，怀孕周假设今天婴儿出生，有多少婴儿-单身，双胞胎等等，和婴儿的性别，如果它知道。护士点击“预测” ，机器学习模型运行，护士得到预测7.19磅或4.36磅，根据投入，然后护士安排特殊设施的婴儿在右边，这是它的工作方式。所以，这就是我们要建造的。对于机器学习，我们需要训练数据。在我们的案例中，美国政府多年来一直在收集出生率的统计数据。该数据可作为 BigQuery 中的示例数据集使用。它的大小合理，它有大约1.4亿行，22g 的数据。我们可以使用这个数据集来建立一个机器学习模型。实际上，当然，你不想使用这么老的数据，1969年到2008年，但是让我们忽略这样一个事实，样本数据集在2008年停止，因为这是一个学习的机会。</p>
<blockquote>
<p>The dataset includes a variety of details about the baby and about the pregnancy. We’ll ignore the birthday, of course, but columns like the US State, the mother’s age, gestation weeks et cetera, those might be useful features. The baby’s birth weight in pounds, that is the label, it is what we’re training our model to predict. Our first step will be to explore this dataset, primarily by visualizing various things. But before that, a quick word on how to access the lab environment.</p>
<p>这个数据集包括了关于婴儿和怀孕的各种细节。当然，我们会忽略生日，但是像美国这样的专栏，母亲的年龄，怀孕周等等，这些可能是有用的功能。婴儿的出生体重，以磅为单位，这是标签，这是我们训练模型来预测的。我们的第一步将是探索这个数据集，主要是通过可视化各种事物。但在此之前，我们先简单介绍一下如何进入实验室环境。</p>
</blockquote>
<p>BigQuery</p>
<p>![1574863195993](./End-to-End Machine Learning with TensorFlow on GCP.assets/1574863195993.png)</p>
<blockquote>
<p>The dataset of births is in BigQuery, the data warehouse on Google Cloud Platform. Let’s do a quick review of what BigQuery is and how you can use it for data exploration. BigQuery is a serverless data warehouse that operates at massive scale. It’s serverless. To use BigQuery, you don’t have to store your data in a cluster. To query the data, you don’t need a massive machine either. All you need is an API call. You invoke BigQuery from just a web browser. You can analyze terabytes to petabytes of data, and it won’t take you hours. Your query will often finished in a few seconds to a couple of minutes. The queries that you write are in a familiar SQL 2011 query language. There are many ways to ingest, transform, load, export data to and from BigQuery. You can ingest CSV, JSON, Avro, Google Sheets, et cetera, and you can also export to those formats. Usually, tables in BigQuery are in denormalized form. In other words, they’re flat, but BigQuery also supports nested and repeated fields, and this is why it can support, for example, JSON because Jason is a hierarchical format. In BigQuery, storage and compute are separate. So you’ll pay a low cost for storage and pay for what you use. A flat rate pricing is also available, but most people go for the on-demand pricing model. To run a BigQuery query, simply visit the BigQuery web page, bigquery.cloud.google.com, and type in your SQL query and hit Run Query. Before running a query, you can click on the Validate button to see how much data would get processed. Queries are charged based on the amount of data processed. Everything that you can do with a web console can also be done with a Python client. So, options include the destination BigQuery table where they are to cash, et cetera. You will get to look at this in the lab, but if you want to copy and paste and try out a query, try out a query on the query that’s on the next slide. So, here’s a quick demo. So, here I’m going to go to console or bigquery.cloud.google.com. I’m inside BigQuery. So let me just move the window a little bit so you can see it. So there you are. You’re in BigQuery. I’ll go ahead and say composed query, and I’ll pick the query from here.</p>
</blockquote>
<p>出生的数据集在 BigQuery 中，这是 Google 云平台上的数据仓库。让我们快速回顾一下什么是 BigQuery，以及如何使用它进行数据探索。Big query 是一个无服务器的数据仓库，可以大规模运行。它是无服务器的。要使用 BigQuery，您不必将数据存储在集群中。要查询数据，你也不需要大型机器。您所需要的只是一个 API 调用。</p>
<p>![1574863403179](./End-to-End Machine Learning with TensorFlow on GCP.assets/1574863403179.png)</p>
<p>![1574863500730](./End-to-End Machine Learning with TensorFlow on GCP.assets/1574863500730.png)</p>
<p>![1574863510170](./End-to-End Machine Learning with TensorFlow on GCP.assets/1574863510170.png)</p>
<p>您只需从一个 web 浏览器调用 BigQuery。你可以分析太字节到千兆字节的数据，这不会花费你几个小时。您的查询通常会在几秒钟到几分钟内完成。您编写的查询使用熟悉的 SQL 2011查询语言。有许多方法可以向 BigQuery 摄取、转换、加载和导出数据。你可以摄取 CSV，JSON，Avro，Google Sheets，等等，你也可以导出到这些格式。通常，BigQuery 中的表是非规范化的。换句话说，它们是扁平的，但是 BigQuery 也支持嵌套和重复字段，这就是为什么它能够支持 JSON，例如，因为 Jason 是一种层次结构格式。在 BigQuery 中，存储和计算是分开的。因此，你将支付一个低成本的存储和支付你使用。固定价格也是可行的，但大多数人选择按需定价模式。要运行一个 BigQuery 查询，只需访问 BigQuery 网页， BigQuery.cloud.google.com  ，然后输入你的 SQL 查询并点击 Run Query。在运行查询之前，可以单击 Validate 按钮查看将处理多少数据。查询是根据处理的数据量收费的。您可以使用 web 控制台完成的任何事情也可以使用 Python 客户机完成。因此，选项包括目标 BigQuery 表，它们将在其中兑现，等等。您将可以在实验室中查看这个查询，但是如果您想要复制、粘贴并尝试查询，请尝试对下一张幻灯片中的查询执行查询。所以，这里有一个简短的演示。所以，在这里我要去控制台或 bigquery.cloud.google.com。我进入了 BigQuery。所以让我把窗户移动一点点，这样你们就能看到了。原来如此。你在 BigQuery。我将继续并说出组合查询，然后从这里选择查询。</p>
<blockquote>
<p>So, here’s a query. It’s a standard sql query. I’m basically going ahead and selecting a couple of columns from this particular table, grouping it by date, and ordering it by the total claim in seconds. Then I’ll go ahead and run the query. This is our standard sql. No spaces there. I’ll run the query. There we go. It turns out that California had 116 million claims and Florida had 91 million claims, et cetera. The point being that we are able to process a dataset with millions of rows and we were able to do this query in less than three seconds.</p>
</blockquote>
<p>所以，这里有一个Query。这是一个标准 sql 查询。基本上，我将继续前进，从这个特定的表中选择几个列，按日期对它进行分组，并按总索赔额(以秒为单位)排序。然后我将继续运行查询。这是我们的标准 sql。这里没有空格。我来运行查询。好了。结果显示，加利福尼亚有1.16亿份申请，佛罗里达有9100万份申请，等等。关键在于我们能够处理数百万行的数据集，而且我们能够在不到三秒钟的时间内完成这个查询。</p>
<h3 id="3-AI-Platform-Notebooks"><a href="#3-AI-Platform-Notebooks" class="headerlink" title="3. AI Platform Notebooks"></a>3. AI Platform Notebooks</h3><p>Besides BigQuery, the other piece of technology you will use in the first lab is AI Platform Notebooks. AI Platform Notebooks is the next generation of hosted notebook on GCP, and has replaced Cloud Datalab. This is a managed Jupyter notebook environment that you can use to run Python code. It handles Besides BigQuery, the other piece of technology you will use in the first lab is AI Platform Notebooks. AI Platform Notebooks is the next generation of hosted notebook on GCP, and has replaced Cloud Datalab. This is a managed Jupyter notebook environment that you can use to run Python code. It handles authentication to GCP, so that you can easily access BigQuery.</p>
<blockquote>
<p>除了 BigQuery，你将在第一个实验室使用的另一项技术是 AI 平台笔记本。Ai 平台笔记本是 GCP 上的下一代托管笔记本，已经取代了 Cloud Datalab。这是一个托管 Jupyter 笔记本环境，您可以使用它来运行 Python 代码。除了 BigQuery，你将在第一个实验室使用的另一项技术是 AI 平台笔记本。Ai 平台笔记本是 GCP 上的下一代托管笔记本，已经取代了 Cloud Datalab。这是一个托管 Jupyter 笔记本环境，您可以使用它来运行 Python 代码。它处理对 GCP 的身份验证，这样您就可以轻松地访问 BigQuery。</p>
</blockquote>
<p>![img](./End-to-End Machine Learning with TensorFlow on GCP.assets/qP0TKaS-Eemn4xL11vEQ3A_b708a3fd9166a17c09bab5b168a178e7_Screen-Shot-2019-07-12-at-9.02.49-AM.png)</p>
<p>AI Platform Notebooks are developed in an iterative collaborative process. You can write code in Python, hit the run button, and the output shows up right on the page itself. Along with the code, you can write commentary in Markdown format and share the notebook with your colleagues.</p>
<blockquote>
<p>Ai 平台笔记本是在一个迭代的协作过程中开发的。您可以用 Python 编写代码，点击运行按钮，输出就会显示在页面上。在编写代码的同时，你还可以用 Markdown 格式写评论，并与同事共享笔记本。</p>
</blockquote>
<p>![img](./End-to-End Machine Learning with TensorFlow on GCP.assets/OKt0WqS_EemW8A5odpwTWA_6385bd2483273f5d620bc922895d7bd5_Screen-Shot-2019-07-12-at-9.07.42-AM.png)</p>
<p>This is what the interface looks like. Notice how there are code sections interleaved with markup and output. This interleaving is what makes this style of computing so useful. Data analysis and machine learning are commonly carried out in notebooks like this. The code is in the blue section. You can execute the code by either clicking the Run button or by pressing Shift + Enter. The red section displays output from the command. Notice that the output here isn’t just command line output; it’s charts and tables as well. The yellow section contains markup, so you can explain why you’re doing what you’re doing in plain-text.</p>
<blockquote>
<p>这就是界面的样子。请注意代码段是如何与标记和输出交叉的。这种交织使得这种计算风格如此有用。数据分析和机器学习通常在这样的笔记本中进行。代码在蓝色部分。您可以通过单击 Run 按钮或按下 Shift + Enter 来执行代码。红色部分显示命令的输出。请注意，这里的输出不仅仅是命令行输出，还有图表和表格。黄色部分包含标记，因此您可以解释为什么要用纯文本进行这些操作。</p>
</blockquote>
<p>![img](./End-to-End Machine Learning with TensorFlow on GCP.assets/xFEelaS_EemLSgpXQLYWKg_ee2255569c5eda592edd09fcfc2b7d5d_Screen-Shot-2019-07-12-at-9.10.41-AM.png)</p>
<p>AI Platform Notebooks work with the same technologies that you’re comfortable with, so you can start developing now, and then work on scale later. For example, we’ll be doing an exercise where we read from a .csv file. You could then process in Pandas and Apache Beam before training a model in TensorFlow, and then improve the model through training.</p>
<p>Eventually though, when you are ready to scale, you can use Google Cloud Storage to hold your data initially, process it in Cloud Dataflow on an ephemeral cluster, and then run distributed training and hyper-parameter optimization in Cloud AI Platform.authentication to GCP, so that you can easily access BigQuery.</p>
<blockquote>
<p>AI 平台笔记本电脑使用的技术和你熟悉的技术一样，所以你可以现在开始开发，然后再进行大规模的工作。例如，我们会做一个练习，我们从一个Csv 文件。然后，您可以在 Pandas 和 Apache Beam 中进行处理，然后在 TensorFlow 中训练模型，然后通过训练改进模型。</p>
<p>最终，当你准备好扩展时，你可以使用谷歌云存储来保存你的数据，在云数据流中处理它，然后在云 AI 平台上运行分布式训练和超参数优化身份验证到 GCP，以便您可以方便地访问 BigQuery。</p>
</blockquote>
<h3 id="GCP"><a href="#GCP" class="headerlink" title="GCP"></a>GCP</h3><p><a href="https://googlecoursera.qwiklabs.com/focuses/35215" target="_blank" rel="noopener">https://googlecoursera.qwiklabs.com/focuses/35215</a></p>
<p>![1575305038496](./End-to-End Machine Learning with TensorFlow on GCP.assets/1575305038496.png)</p>
<p>![1575305055448](./End-to-End Machine Learning with TensorFlow on GCP.assets/1575305055448.png)</p>
<p>![1575305065467](End-to-End Machine Learning with TensorFlow on GCP.assets/1575305065467.png)</p>
<p>![1575305073525](End-to-End Machine Learning with TensorFlow on GCP.assets/1575305073525.png)</p>
<p>![1575305084379](./End-to-End Machine Learning with TensorFlow on GCP.assets/1575305084379.png)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/21/How%20to%20train%20your%20Deep%20Neural%20Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vellichor">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vellichor Kyst">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/21/How%20to%20train%20your%20Deep%20Neural%20Network/" itemprop="url">How to train your Deep Neural Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-21T18:01:57+08:00">
                2019-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="How-to-train-your-Deep-Neural-Network"><a href="#How-to-train-your-Deep-Neural-Network" class="headerlink" title="How to train your Deep Neural Network"></a>How to train your Deep Neural Network</h1><p>Jan 5, 2017</p>
<p>There are certain practices in <strong>Deep Learning</strong> that are highly recommended, in order to efficiently train <strong>Deep Neural Networks</strong>. In this post, I will be covering a few of these most commonly used practices, ranging from importance of quality training data, choice of hyperparameters to more general tips for faster prototyping of DNNs. Most of these practices, are validated by the research in academia and industry and are presented with mathematical and experimental proofs in research papers like <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">Efficient BackProp(Yann LeCun et al.)</a> and <a href="https://arxiv.org/pdf/1206.5533v2.pdf" target="_blank" rel="noopener">Practical Recommendations for Deep Architectures(Yoshua Bengio)</a>.</p>
<p>As you’ll notice, I haven’t mentioned any mathematical proofs in this post. All the points suggested here, should be taken more of a summarization of the best practices for training DNNs. For more in-depth understanding, I highly recommend you to go through the above mentioned research papers and references provided at the end.</p>
<hr>
<h3 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h3><p>A lot of ML practitioners are habitual of throwing raw training data in any <strong>Deep Neural Net(DNN)</strong>. And why not, any DNN would(presumably) still give good results, right? But, it’s not completely old school to say that - “given the right type of data, a fairly simple model will provide better and faster results than a complex DNN”(although, this might have exceptions). So, whether you are working with <strong>Computer Vision</strong>, <strong>Natural Language Processing</strong>, <strong>Statistical Modelling</strong>, etc. try to preprocess your raw data. A few measures one can take to get better training data:</p>
<ul>
<li>Get your hands on as large a dataset as possible(DNNs are quite data-hungry: more is better)</li>
<li>Remove any training sample with corrupted data(short texts, highly distorted images, spurious output labels, features with lots of null values, etc.)</li>
<li>Data Augmentation - create new examples(in case of images - rescale, add noise, etc.)</li>
</ul>
<h3 id="Choose-appropriate-activation-functions"><a href="#Choose-appropriate-activation-functions" class="headerlink" title="Choose appropriate activation functions"></a>Choose appropriate activation functions</h3><p>One of the vital components of any Neural Net are <a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="noopener">activation functions</a>. <strong>Activations</strong> introduces the much desired <strong>non-linearity</strong> into the model. For years, <code>sigmoid</code> activation functions have been the preferable choice. But, a <code>sigmoid</code> function is inherently cursed by these two drawbacks - 1. Saturation of sigmoids at tails(further causing <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" target="_blank" rel="noopener">vanishing gradient problem</a>). 2. <code>sigmoids</code> are not zero-centered.</p>
<p>A better alternative is a <code>tanh</code> function - mathematically, <code>tanh</code> is just a rescaled and shifted <code>sigmoid</code>, <code>tanh(x) = 2*sigmoid(x) - 1</code>. Although <code>tanh</code> can still suffer from the <strong>vanishing gradient problem</strong>, but the good news is - <code>tanh</code> is zero-centered. Hence, using <code>tanh</code> as activation function will result into faster convergence. I have found that using <code>tanh</code> as activations generally works better than sigmoid.</p>
<p>You can further explore other alternatives like <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener"><code>ReLU</code></a>, <code>SoftSign</code>, etc. depending on the specific task, which have shown to ameliorate some of these issues.</p>
<h3 id="Number-of-Hidden-Units-and-Layers"><a href="#Number-of-Hidden-Units-and-Layers" class="headerlink" title="Number of Hidden Units and Layers"></a>Number of Hidden Units and Layers</h3><p>Keeping a larger number of hidden units than the optimal number, is generally a safe bet. Since, any regularization method will take care of superfluous units, at least to some extent. On the other hand, while keeping smaller numbers of hidden units(than the optimal number), there are higher chances of underfitting the model.</p>
<p>Also, while employing <strong>unsupervised pre-trained representations</strong>(describe in later sections), the optimal number of hidden units are generally kept even larger. Since, pre-trained representation might contain a lot of irrelevant information in these representations(for the specific supervised task). By increasing the number of hidden units, model will have the required flexibility to filter out the most appropriate information out of these pre-trained representations.</p>
<p>Selecting the optimal number of layers is relatively straight forward. As <a href="https://www.quora.com/profile/Yoshua-Bengio" target="_blank" rel="noopener">@Yoshua-Bengio</a> mentioned on Quora - “You just keep on adding layers, until the test error doesn’t improve anymore”. ;)</p>
<h3 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h3><p>Always initialize the weights with small <code>random numbers</code> to break the symmetry between different units. But how small should weights be? What’s the recommended upper limit? What probability distribution to use for generating random numbers? Furthermore, while using <code>sigmoid</code> activation functions, if weights are initialized to very large numbers, then the sigmoid will <strong>saturate</strong>(tail regions), resulting into <strong>dead neurons</strong>. If weights are very small, then gradients will also be small. Therefore, it’s preferable to choose weights in an intermediate range, such that these are distributed evenly around a mean value.</p>
<p>Thankfully, there has been lot of research regarding the appropriate values of initial weights, which is really important for an efficient convergence. To initialize the weights that are evenly distributed, a <code>uniform distribution</code> is probably one of the best choice. Furthermore, as shown in the <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">paper(Glorot and Bengio, 2010)</a>, units with more incoming connections(fan_in) should have relatively smaller weights.</p>
<p>Thanks to all these thorough experiments, now we have a tested formula that we can directly use for weight initialization; i.e. - weights drawn from <code>~ Uniform(-r, r)</code> where <code>r=sqrt(6/(fan_in+fan_out))</code> for <code>tanh</code> activations, and <code>r=4*(sqrt(6/fan_in+fan_out))</code> for <code>sigmoid</code> activations, where <code>fan_in</code> is the size of the previous layer and <code>fan_out</code> is the size of next layer.</p>
<h3 id="Learning-Rates"><a href="#Learning-Rates" class="headerlink" title="Learning Rates"></a>Learning Rates</h3><p>This is probably one of the most important hyperparameter, governing the learning process. Set the learning rate too small and your model might take ages to converge, make it too large and within initial few training examples, your loss might shoot up to sky. Generally, a learning rate of <code>0.01</code> is a safe bet, but this shouldn’t be taken as a stringent rule; since the optimal learning rate should be in accordance to the specific task.</p>
<p>In contrast to, a fixed learning rate, gradually decreasing the learning rate, after each epoch or after a few thousand examples is another option. Although this might help in faster training, but requires another manual decision about the new learning rates. Generally, <strong>learning rate can be halved after each epoch</strong> - these kinds of strategies were quite common a few years back.</p>
<p>Fortunately, now we have better <code>momentum based methods</code> to change the learning rate, based on the curvature of the error function. It might also help to set different learning rates for individual parameters in the model; since, some parameters might be learning at a relatively slower or faster rate.</p>
<p>Lately, there has been a good amount of research on optimization methods, resulting into <code>adaptive learning rates</code>. At this moment, we have numerous options starting from good old <code>Momentum Method</code> to <code>Adagrad</code>, <code>Adam</code>(personal favourite ;)), <code>RMSProp</code> etc. Methods like <code>Adagrad</code> or <code>Adam</code>, effectively save us from manually choosing an <code>initial learning rate</code>, and given the right amount of time, the model will start to converge quite smoothly(of course, still selecting a good initial rate will further help).</p>
<h3 id="Hyperparameter-Tuning-Shun-Grid-Search-Embrace-Random-Search"><a href="#Hyperparameter-Tuning-Shun-Grid-Search-Embrace-Random-Search" class="headerlink" title="Hyperparameter Tuning: Shun Grid Search - Embrace Random Search"></a>Hyperparameter Tuning: Shun Grid Search - Embrace Random Search</h3><p><strong>Grid Search</strong> has been prevalent in classical machine learning. But, Grid Search is not at all efficient in finding optimal hyperparameters for DNNs. Primarily, because of the time taken by a DNN in trying out different hyperparameter combinations. As the number of hyperparameters keeps on increasing, computation required for Grid Search also increases exponentially.</p>
<p>There are two ways to go about it:</p>
<ol>
<li>Based on your prior experience, you can manually tune some common hyperparameters like learning rate, number of layers, etc.</li>
<li>Instead of Grid Search, use <strong>Random Search/Random Sampling</strong> for choosing optimal hyperparameters. A combination of hyperparameters is generally choosen from a <strong>uniform distribution</strong> within the desired range. It is also possible to add some prior knowledge to further decrease the search space(like learning rate shouldn’t be too large or too small). Random Search has been found to be way more efficient compared to Grid Search.</li>
</ol>
<h3 id="Learning-Methods"><a href="#Learning-Methods" class="headerlink" title="Learning Methods"></a>Learning Methods</h3><p>Good old <strong>Stochastic Gradient Descent</strong> might not be as efficient for DNNs(again, not a stringent rule), lately there have been a lot of research to develop more flexible optimization algorithms. For e.g.: <code>Adagrad</code>, <code>Adam</code>, <code>AdaDelta</code>, <code>RMSProp</code>, etc. In addition to providing <strong>adaptive learning rates</strong>, these sophisticated methods also use <strong>different rates for different model parameters</strong> and this generally results into a smoother convergence. It’s good to consider these as hyper-parameters and one should always try out a few of these on a subset of training data.</p>
<h3 id="Keep-dimensions-of-weights-in-the-exponential-power-of-2"><a href="#Keep-dimensions-of-weights-in-the-exponential-power-of-2" class="headerlink" title="Keep dimensions of weights in the exponential power of 2"></a>Keep dimensions of weights in the exponential power of 2</h3><p>Even, when dealing with <strong>state-of-the-art</strong> Deep Learning Models with latest hardware resources, <strong>memory management</strong> is still done at the byte level; So, it’s always good to keep the size of your parameters as <code>64</code>, <code>128</code>, <code>512</code>, <code>1024</code>(all powers of <code>2</code>). This might help in sharding the matrices, weights, etc. resulting into slight boost in learning efficiency. This becomes even more significant when dealing with <strong>GPUs</strong>.</p>
<h3 id="Unsupervised-Pretraining"><a href="#Unsupervised-Pretraining" class="headerlink" title="Unsupervised Pretraining"></a>Unsupervised Pretraining</h3><p>Doesn’t matter whether you are working with NLP, Computer Vision, Speech Recognition, etc. <strong>Unsupervised Pretraining</strong> always help the training of your supervised or other unsupervised models. <strong>Word Vectors</strong> in NLP are ubiquitous; you can use <a href="http://image-net.org/" target="_blank" rel="noopener">ImageNet</a> dataset to pretrain your model in an unsupervised manner, for a 2-class supervised classification; or audio samples from a much larger domain to further use that information for a speaker disambiguation model.</p>
<h3 id="Mini-Batch-vs-Stochastic-Learning"><a href="#Mini-Batch-vs-Stochastic-Learning" class="headerlink" title="Mini-Batch vs. Stochastic Learning"></a>Mini-Batch vs. Stochastic Learning</h3><p>Major objective of training a model is to learn appropriate parameters, that results into an optimal mapping from inputs to outputs. These parameters are tuned with each training sample, irrespective of your decision to use <strong>batch</strong>, <strong>mini-batch</strong> or <strong>stochastic learning</strong>. While employing a stochastic learning approach, gradients of weights are tuned after each training sample, introducing noise into gradients(hence the word ‘stochastic’). This has a very desirable effect; i.e. - with the introduction of <strong>noise</strong> during the training, the model becomes less prone to overfitting.</p>
<p>However, going through the stochastic learning approach might be relatively less efficient; since now a days machines have far more computation power. Stochastic learning might effectively waste a large portion of this. If we are capable of computing <strong>Matrix-Matrix multiplication</strong>, then why should we limit ourselves, to iterate through the multiplications of individual pairs of <strong>Vectors</strong>? Therefore, for greater throughput/faster learning, it’s recommended to use mini-batches instead of stochastic learning.</p>
<p>But, selecting an appropriate batch size is equally important; so that we can still retain some noise(by not using a huge batch) and simultaneously use the computation power of machines more effectively. Commonly, a batch of <code>16</code> to <code>128</code> examples is a good choice(exponential of <code>2</code>). Usually, batch size is selected, once you have already found more important hyperparameters(by <strong>manual search</strong> or <strong>random search</strong>). Nevertheless, there are scenarios when the model is getting the training data as a stream(<a href="https://en.wikipedia.org/wiki/Online_machine_learning" target="_blank" rel="noopener">online learning</a>), then resorting to Stochastic Learning is a good option.</p>
<h3 id="Shuffling-training-examples"><a href="#Shuffling-training-examples" class="headerlink" title="Shuffling training examples"></a>Shuffling training examples</h3><p>This comes from <strong>Information Theory</strong> - “Learning that an unlikely event has occurred is more informative than learning that a likely event has occurred”. Similarly, randomizing the order of training examples(in different epochs, or mini-batches) will result in faster convergence. A slight boost is always noticed when the model doesn’t see a lot of examples in the same order.</p>
<h3 id="Dropout-for-Regularization"><a href="#Dropout-for-Regularization" class="headerlink" title="Dropout for Regularization"></a>Dropout for Regularization</h3><p>Considering, millions of parameters to be learned, regularization becomes an imperative requisite to prevent <strong>overfitting</strong> in DNNs. You can keep on using <strong>L1/L2</strong> regularization as well, but <strong>Dropout</strong> is preferable to check overfitting in DNNs. Dropout is trivial to implement and generally results into faster learning. A default value of <code>0.5</code> is a good choice, although this depends on the specific task,. If the model is less complex, then a dropout of <code>0.2</code> might also suffice.</p>
<p>Dropout should be turned off, during the test phase, and weights should be scaled accordingly, as done in the <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener">original paper</a>. Just allow a model with Dropout regularization, a little bit more training time; and the error will surely go down.</p>
<h3 id="Number-of-Epochs-Training-Iterations"><a href="#Number-of-Epochs-Training-Iterations" class="headerlink" title="Number of Epochs/Training Iterations"></a>Number of Epochs/Training Iterations</h3><p>“Training a Deep Learning Model for multiple epochs will result in a better model” - we have heard it a couple of times, but how do we quantify “many”? Turns out, there is a simple strategy for this - Just keep on training your model for a fixed amount of examples/epochs, let’s say <code>20,000</code> examples or <code>1</code> epoch. After each set of these examples compare the <strong>test error</strong> with <strong>train error</strong>, if the gap is decreasing, then keep on training. In addition to this, after each such set, save a copy of your model parameters(so that you can choose from multiple models once it is trained).</p>
<h3 id="Visualize"><a href="#Visualize" class="headerlink" title="Visualize"></a>Visualize</h3><p>There are a thousand ways in which the training of a deep learning model might go wrong. I guess we have all been there, when the model is being trained for hours or days and only after the training is finished, we realize something went wrong. In order to save yourself from bouts of hysteria, in such situations(which might be quite justified ;)) - <strong>always visualize the training process</strong>. Most obvious step you can take is to <strong>print/save logs</strong> of <code>loss</code> values, <code>train error</code> or <code>test error</code>, etc.</p>
<p>In addition to this, another good practice is to use a visualization library to plot histograms of weights after few training examples or between epochs. This might help in keeping track of some of the common problems in Deep Learning Models like <strong>Vanishing Gradient</strong>, <strong>Exploding Gradient</strong> etc.</p>
<h3 id="Multi-Core-machines-GPUs"><a href="#Multi-Core-machines-GPUs" class="headerlink" title="Multi-Core machines, GPUs"></a>Multi-Core machines, GPUs</h3><p>Advent of GPUs, libraries that provide vectorized operations, machines with more computation power, are probably some of the most significant factors in the success of Deep Learning. If you think, you are patient as a stone, you might try running a DNN on your laptop(which can’t even open 10 tabs in your Chrome browser) and wait for ages to get your results. Or you can play smart(and expensively :z) and get a descent hardware with at least <strong>multiple CPU cores</strong> and a <strong>few hundred GPU cores</strong>. GPUs have revolutionized the Deep Learning research(no wonder Nvidia’s stocks are shooting up ;)), primarily because of their ability to perform Matrix Operations at a larger scale.</p>
<p>So, instead of taking weeks on a normal machine, these parallelization techniques, will bring down the training time to days, if not hours.</p>
<h3 id="Use-libraries-with-GPU-and-Automatic-Differentiation-Support"><a href="#Use-libraries-with-GPU-and-Automatic-Differentiation-Support" class="headerlink" title="Use libraries with GPU and Automatic Differentiation Support"></a>Use libraries with GPU and Automatic Differentiation Support</h3><p>Thankfully, for rapid prototyping we have some really descent libraries like <a href="http://deeplearning.net/software/theano/" target="_blank" rel="noopener">Theano</a>, <a href="https://www.tensorflow.org/" target="_blank" rel="noopener">Tensorflow</a>, <a href="https://keras.io/" target="_blank" rel="noopener">Keras</a>, etc. Almost all of these DL libraries provide <strong>support for GPU computation</strong> and <strong>Automatic Differentiation</strong>. So, you don’t have to dive into core GPU programming(unless you want to - it’s definitely fun :)); nor you have to write your own differentiation code, which might get a little bit taxing in really complex models(although you should be able to do that, if required). Tensorflow further provides support for training your models on a <strong>distributed architecture</strong>(if you can afford it).</p>
<p>This is not at all an exhaustive list of practices, to train a DNN. In order to include just the most common practices, I have tried to exclude a few concepts like Normalization of inputs, Batch/Layer Normalization, Gradient Check, etc. Although feel free to add anything in the comment section and I’ll be more than happy to update it in the post. :)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/12/Git%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vellichor">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vellichor Kyst">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/12/Git%E7%9A%84%E4%BD%BF%E7%94%A8/" itemprop="url">Git的使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-12T15:27:54+08:00">
                2019-11-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Git的使用"><a href="#Git的使用" class="headerlink" title="Git的使用"></a>Git的使用</h1><h2 id="I-使用Git-amp-GitHubRepo-amp-Typora构建笔记"><a href="#I-使用Git-amp-GitHubRepo-amp-Typora构建笔记" class="headerlink" title="I. 使用Git&amp;GitHubRepo&amp;Typora构建笔记"></a>I. 使用Git&amp;GitHubRepo&amp;Typora构建笔记</h2><h5 id="1-在typora偏好设置中选中存入图片复制到assets文件夹中"><a href="#1-在typora偏好设置中选中存入图片复制到assets文件夹中" class="headerlink" title="1.在typora偏好设置中选中存入图片复制到assets文件夹中"></a>1.在typora偏好设置中选中存入图片复制到assets文件夹中</h5><p>并且设置为<strong>优先使用相关路径</strong>（负责在Github中Typora仍然无法顺利显示图片）</p>
<blockquote>
<h6 id=""><a href="#" class="headerlink" title=""></a><img src="./Git%E7%9A%84%E4%BD%BF%E7%94%A8.assets/1570787625890.png" alt="1570787625890"></h6></blockquote>
<h5 id="2-在整个存放TyporaNote与Assets的文件夹外建立git"><a href="#2-在整个存放TyporaNote与Assets的文件夹外建立git" class="headerlink" title="2.在整个存放TyporaNote与Assets的文件夹外建立git"></a>2.在整个存放TyporaNote与Assets的文件夹外建立git</h5><p>安装git之后右键文件夹打开git bash here</p>
<p><img src="./Git%E7%9A%84%E4%BD%BF%E7%94%A8.assets/1570785291509.png" alt="1570785291509"></p>
<p>使用git时,先设置</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;git config --<span class="keyword">global</span> user.name <span class="string">"Eviliclufas"</span></span><br><span class="line">&gt;git config --<span class="keyword">global</span> user.email <span class="string">"123@outlook.com"</span></span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m <span class="string">"first commit"</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="3-在github建立新仓库"><a href="#3-在github建立新仓库" class="headerlink" title="3.在github建立新仓库"></a>3.在github建立新仓库</h5><p><img src="./Git%E7%9A%84%E4%BD%BF%E7%94%A8.assets/1570785378922.png" alt="1570785378922"></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;git remote add origin https://github.com/EvilicLufas/TyporaAssets.git</span><br><span class="line">&gt;git push -u origin master</span><br></pre></td></tr></table></figure>
</blockquote>
<p>至此你的Typora笔记以及Assets文件夹都已经上传到了github的仓库里</p>
<p><img src="./Git%E7%9A%84%E4%BD%BF%E7%94%A8.assets/1570785581460.png" alt="1570785581460"></p>
<h5 id="4-该过程中可能出现的问题（在Github创建库时不要选择创建readme）"><a href="#4-该过程中可能出现的问题（在Github创建库时不要选择创建readme）" class="headerlink" title="4.该过程中可能出现的问题（在Github创建库时不要选择创建readme）"></a>4.该过程中可能出现的问题（在Github创建库时不要选择创建readme）</h5><blockquote>
<p>git pull 失败 ,提示：<code>fatal: refusing to merge unrelated histories</code></p>
<p>其实这个问题是因为 两个 根本不相干的 git 库， 一个是本地库， 一个是远端库， 然后本地要去推送到远端， 远端觉得这个本地库跟自己不相干， 所以告知无法合并</p>
<p>具体的方法， 一个种方法： 是 从远端库拉下来代码 ， 本地要加入的代码放到远端库下载到本地的库， 然后提交上去 ， 因为这样的话， 你基于的库就是远端的库， 这是一次<a href="https://www.centos.bz/tag/update/" target="_blank" rel="noopener">update</a>了</p>
<p>第二种方法：<br>使用这个强制的方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master --allow-unrelated-histories</span><br></pre></td></tr></table></figure>

<p>后面加上 <code>--allow-unrelated-histories</code> ， 把两段不相干的 分支进行强行合并</p>
<p>后面再push就可以了 <code>git push gitlab master:init</code></p>
<p>gitlab是别名 ， 使用</p>
<p>Java代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add gitlab ssh://xzh@192.168.1.91:50022/opt/gitrepo/withholdings/WithholdingTransaction`</span><br></pre></td></tr></table></figure>

<p>master是本地的branch名字<br>init是远端要推送的branch名字</p>
<p>本地必须要先add ，<a href="https://www.centos.bz/tag/commit/" target="_blank" rel="noopener">commit</a>完了 才能推上去</p>
<p>关于这个问题，可以参考http://<a href="https://www.centos.bz/2018/03/git-出现-fatal-refusing-to-merge-unrelated-histories-错误/#" target="_blank" rel="noopener">stack</a>overflow.com/questions/37937984/git-refusing-to-merge-unrelated-histories。</p>
<p>在进行git pull 时，添加一个可选项</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master --allow-unrelated-histories</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="5-git-init-之后选择隐藏的项目-才能看到-git文件夹"><a href="#5-git-init-之后选择隐藏的项目-才能看到-git文件夹" class="headerlink" title="5.git init 之后选择隐藏的项目 才能看到.git文件夹"></a>5.git init 之后选择隐藏的项目 才能看到.git文件夹</h5><blockquote>
<p><img src="./Git%E7%9A%84%E4%BD%BF%E7%94%A8.assets/1570785929130.png" alt="1570785929130"></p>
</blockquote>
<h2 id="II-使用Git更新项目到本地仓库"><a href="#II-使用Git更新项目到本地仓库" class="headerlink" title="II. 使用Git更新项目到本地仓库"></a>II. 使用Git更新项目到本地仓库</h2><h5 id="1-使用git-fetch更新，相当于是从远程获取最新版本到本地，不会自动merge"><a href="#1-使用git-fetch更新，相当于是从远程获取最新版本到本地，不会自动merge" class="headerlink" title="1.使用git fetch更新，相当于是从远程获取最新版本到本地，不会自动merge"></a>1.使用git fetch更新，相当于是从远程获取最新版本到本地，不会自动merge</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">git fetch origin master</span><br><span class="line">git log -p master..origin/master</span><br><span class="line">git merge origin/master</span><br></pre></td></tr></table></figure>

<p>首先从远程的origin的master主分支下载最新的版本到origin/master分支上<br>然后比较本地的master分支和origin/master分支的差别<br>最后进行合并<br>上述过程其实可以用以下更清晰的方式来进行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git fetch origin master:tmp</span><br><span class="line">git diff tmp </span><br><span class="line">git merge tmp</span><br></pre></td></tr></table></figure>

<p>从远程获取最新的版本到本地的tmp分支上<br>之后再进行比较合并</p>
<h5 id="2-使用git-pull-更新，相当于是从远程获取最新版本并merge到本地"><a href="#2-使用git-pull-更新，相当于是从远程获取最新版本并merge到本地" class="headerlink" title="2.使用git pull 更新，相当于是从远程获取最新版本并merge到本地"></a>2.使用git pull 更新，相当于是从远程获取最新版本并merge到本地</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure>

<p>上述命令其实相当于git fetch 和 git merge<br>在实际使用中，git fetch更安全一些<br>因为在merge前，我们可以查看更新情况，然后再决定是否合并</p>
<h2 id="III-本地改动导致git-pull出现Error解决方法"><a href="#III-本地改动导致git-pull出现Error解决方法" class="headerlink" title="III. 本地改动导致git pull出现Error解决方法"></a>III. 本地改动导致git pull出现Error解决方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">From https://github.com/EvilicLufas/TyporaAssets</span><br><span class="line"> * branch            master     -&gt; FETCH_HEAD</span><br><span class="line">   278ba10..83d7801  master     -&gt; origin/master</span><br><span class="line">error: Your local changes to the following files would be overwritten by merge:</span><br><span class="line">        typoraNote/待办事项（2019-10-10  到   10 -28）.md</span><br><span class="line">Please commit your changes or stash them before you merge.</span><br><span class="line">Aborting</span><br><span class="line">Updating f90196a..83d7801</span><br></pre></td></tr></table></figure>

<p> 通过git stash将工作区恢复到上次提交的内容，同时备份本地所做的修改，之后就可以正常git pull了，git pull完成后，执行git stash pop将之前本地做的修改应用到当前工作区。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git stash</span><br><span class="line">git pull</span><br><span class="line">git stash pop</span><br></pre></td></tr></table></figure>

<p>git stash: 备份当前的工作区的内容，从最近的一次提交中读取相关内容，让工作区保证和上次提交的内容一致。同时，将当前的工作区内容保存到Git栈中。</p>
<p>git pull:拉取服务器上的代码；</p>
<p>git stash pop: 从Git栈中读取最近一次保存的内容，恢复工作区的相关内容。由于可能存在多个Stash的内容，所以用栈来管理，pop会从最近的一个stash中读取内容并恢复。</p>
<p>git stash list: 显示Git栈内的所有备份，可以利用这个列表来决定从那个地方恢复。</p>
<p>git stash clear: 清空Git栈。此时使用gitg等图形化工具会发现，原来stash的哪些节点都消失了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/11/Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vellichor">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vellichor Kyst">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/11/Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread)/" itemprop="url">Gossip算法的研究与实现(Multi-thread using Java)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-11T20:27:13+08:00">
                2019-10-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Gossip算法的研究与实现-Multi-thread-using-Java"><a href="#Gossip算法的研究与实现-Multi-thread-using-Java" class="headerlink" title="Gossip算法的研究与实现(Multi-thread using Java)"></a>Gossip算法的研究与实现(Multi-thread using Java)</h1><p>[TOC]</p>
<hr>
<h2 id="Chapter-I-Gossip算法介绍"><a href="#Chapter-I-Gossip算法介绍" class="headerlink" title="Chapter I. Gossip算法介绍"></a>Chapter I. Gossip算法介绍</h2><h4 id="1-算法背景"><a href="#1-算法背景" class="headerlink" title="1.算法背景"></a>1.算法背景</h4><p>​        由于卡夫卡集群的特性，在系统运行一段时间后（默认配置是7天），会自动清除掉过期的记录，因此每个周期之后加入的节点都会丢失一部分数据。于是，我们需要一个机制能不依赖卡夫卡集群来实现数据的一致性。</p>
<p>​        这就是 Gossip算法。当卡夫卡集群无法保证数据一致性时，通过此算法，保证系统最终数据一致。同时，还可支持节点间各种类型的消息传播。</p>
<h4 id="2-Gossip算法概述"><a href="#2-Gossip算法概述" class="headerlink" title="2.Gossip算法概述"></a>2.Gossip算法概述</h4><h5 id="2-1-算法简介"><a href="#2-1-算法简介" class="headerlink" title="2.1 算法简介"></a>2.1 算法简介</h5><blockquote>
<p>Gossip, or anti-entropy,  is an attractive way of replicating state that does not have strong consistency requirements</p>
</blockquote>
<p>​        顾名思义，类似于流言传播的概念，Gossip是一种可以按照自己的期望自行选择与之交换信息的节点的通信方式</p>
<p>​        在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。每个节点可能知道所有其他节点，也可能仅知道几个邻居节点，只要这些节可以通过网络连通，最终他们的状态都是一致的，当然这也是疫情传播的特点。</p>
<p>​        Gossip是一种<strong>去中心化</strong>、<strong>容错</strong>而又<strong>最终一致性</strong>的绝妙算法，其收敛性不但得到证明还具有<strong>指数级</strong>的收敛速度。使用 Gossip 的系统可以很容易的将 Server扩展到更多的节点，满足弹性扩展轻而易举。</p>
<p>​        Gossip常见于大规模、无中心的网络系统，可以用于众多能接受“最终一致性”的领域：失败检测、路由同步、Pub/Sub、动态负载均衡。</p>
<h5 id="2-2-算法特点"><a href="#2-2-算法特点" class="headerlink" title="2.2 算法特点"></a>2.2 算法特点</h5><p>​        Gossip不要求节点知道所有其他节点，因此具有<strong>去中心化</strong>的特点，节点之间完全对等，不需要任何的中心节点。</p>
<p>​        Gossip算法又被称为反熵（Anti-Entropy），熵是物理学上的一个概念，代表杂乱无章，而反熵就是在杂乱无章中寻求一致，这充分说明了Gossip的特点：</p>
<blockquote>
<p>在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。每个节点可能知道所有其他节点，也可能仅知道几个邻节点，只要这些节可以通过网络连通，最终他们的状态都是一致的。</p>
</blockquote>
<p>​        Gossip算法是一个最终一致性算法，其无法保证在某个时刻所有节点状态一致，但可以保证在”最终“所有节点一致，<strong>”最终“是一个现实中存在，但理论上无法证明的时间点。</strong></p>
<h5 id="2-3-Gossip协议满足的条件"><a href="#2-3-Gossip协议满足的条件" class="headerlink" title="2.3 Gossip协议满足的条件"></a>2.3 Gossip协议满足的条件</h5><ul>
<li>协议的核心包括周期性，成对性，内部进程交互</li>
<li>交互期间的信息量大小固定</li>
<li>节点交互后，至少一个agent获知另一个agent的状态</li>
<li>通信不可靠</li>
<li>交流的频率远远低于消息的传输延迟</li>
<li>对端选择的随机性，或者从全集，或者从部分集合</li>
<li>由于副本的存在，传输的信息具有隐式冗余</li>
</ul>
<h5 id="2-4-协调机制"><a href="#2-4-协调机制" class="headerlink" title="2.4 协调机制"></a>2.4 协调机制</h5><p>​        协调机制是讨论在每次2个节点通信时，如何交换数据能达到最快的一致性，也即消除两个节点的不一致性。<br>​        协调所面临的最大问题是，因为受限于网络负载，不可能每次都把一个节点上的数据发送给另外一个节点，也即每个Gossip的消息大小都有上限。在有限的空间上有效率地交换所有的消息是协调要解决的主要问题。</p>
<blockquote>
<p>“Efficient Reconciliation and Flow Control for Anti-Entropy Protocols”中描述了两种同步机制<br><strong>1）precise reconciliation</strong></p>
<blockquote>
<p>​        precise reconciliation希望在每次通信周期内都非常准确地消除双方的不一致性，具体表现为相互发送对方需要更新的数据，因为每个节点都在并发与多个节点通信，理论上很难做到。precise reconciliation需要给每个数据项独立地维护自己的version，在每次交互是把所有的(key,value,version)发送到目标进行比对，从而找出双方不同之处从而更新。但因为Gossip消息存在大小限制，因此每次选择发送哪些数据就成了问题。当然可以随机选择一部分数据，也可确定性的选择数据。对确定性的选择而言，可以有最老优先（根据版本）和最新优先两种，最老优先会优先更新版本最新的数据，而最新更新正好相反，这样会造成老数据始终得不到机会更新，也即饥饿。</p>
</blockquote>
<p><strong>2）Scuttlebutt Reconciliation</strong></p>
<blockquote>
<p>​        Scuttlebutt Reconciliation 与precise reconciliation不同之处是，Scuttlebutt Reconciliation不是为每个数据都维护单独的版本号，而是为每个节点上的宿主数据维护统一的version。比如节点P会为(p1,p2,…)维护一个一致的全局version，相当于把所有的宿主数据看作一个整体，当与其他节点进行比较时，只需比较这些宿主数据的最高version，如果最高version相同说明这部分数据全部一致，否则再进行precise reconciliation。</p>
</blockquote>
</blockquote>
<h5 id="2-5-Merkle-tree"><a href="#2-5-Merkle-tree" class="headerlink" title="2.5 Merkle tree"></a>2.5 Merkle tree</h5><p>​        信息同步无疑是gossip的核心，Merkle tree(MT)是一个非常适合同步的数据结构。<br>​        简单来说 Merkle tree就是一颗hash树，在这棵树中，叶子节点的值是一些hash值、非叶节点的值均是由其子节点值计算hash得来的，这样，一旦某个文件被修改，修改时间的信息就会迅速传播到根目录。需要同步的系统只需要不断查询跟节点的hash，一旦有变化，顺着树状结构就能够在 logN 级别的时间找到发生变化的内容，马上同步。<br>​        在Dynamo中，每个节点保存一个范围内的key值，不同节点间存在有相互交迭的key值范围。在去熵操作中，考虑的仅仅是某两个节点间共有的key值范围。MT的叶子节点即是这个共有的key值范围内每个key的hash，通过叶子节点的hash自底向上便可以构建出一颗MT。Dynamo首先比对MT根处的hash，如果一致则表示两者完全一致，否则将其子节点交换并继续比较的过程。</p>
<h5 id="2-6-时间复杂度-logN-的证明"><a href="#2-6-时间复杂度-logN-的证明" class="headerlink" title="2.6 时间复杂度 logN 的证明"></a>2.6 时间复杂度 logN 的证明</h5><p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1570799966173.png" alt="1570799966173"></p>
<p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1570799985330.png" alt="1570799985330"></p>
<h5 id="2-7-算法具体描述"><a href="#2-7-算法具体描述" class="headerlink" title="2.7 算法具体描述"></a>2.7 算法具体描述</h5><h6 id="2-7-1-gossip-协议的类型"><a href="#2-7-1-gossip-协议的类型" class="headerlink" title="2.7.1 gossip 协议的类型"></a>2.7.1 gossip 协议的类型</h6><p>前面说了节点会将信息传播到整个网络中，那么节点在什么情况下发起信息交换？这就涉及到 gossip 协议的类型。目前主要有两种方法：</p>
<p>Anti-Entropy（反熵）：以固定的概率传播所有的数据<br>Rumor-Mongering（谣言传播）：仅传播新到达的数据</p>
<h6 id="2-7-2-Anti-Entropy"><a href="#2-7-2-Anti-Entropy" class="headerlink" title="2.7.2 Anti-Entropy"></a>2.7.2 Anti-Entropy</h6><p>Anti-Entropy 的主要工作方式是：每个节点周期性地随机选择其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异。Anti-Entropy 这种方法非常可靠，但是每次节点两两交换自己的所有数据会带来非常大的通信负担，以此不会频繁使用。</p>
<p>Anti-Entropy 使用“simple epidemics”的方式，所以其包含两种状态：susceptible 和 infective，这种模型也称为 SI model。处于 infective 状态的节点代表其有数据更新，并且会将这个数据分享给其他节点；处于 susceptible 状态的节点代表其并没有收到来自其他节点的更新。</p>
<h6 id="2-7-3-Rumor-Mongering"><a href="#2-7-3-Rumor-Mongering" class="headerlink" title="2.7.3 Rumor-Mongering"></a>2.7.3 Rumor-Mongering</h6><p>Rumor-Mongering 的主要工作方式是：当一个节点有了新的信息后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新信息。直到所有的节点都知道该新信息。因为节点之间只是交换新信息，所有大大减少了通信的负担。</p>
<p>Rumor-Mongering 使用“complex epidemics”方法，相比 Anti-Entropy 多了一种状态：removed，这种模型也称为 SIR model。处于 removed 状态的节点说明其已经接收到来自其他节点的更新，但是其并不会将这个更新分享给其他节点。</p>
<p>因为 Rumor 消息会在某个时间标记为 removed，然后不会发送给其他节点，所以 Rumor-Mongering 类型的 gossip 协议有极小概率使得更新不会达到所有节点。</p>
<p>一般来说，为了在通信代价和可靠性之间取得折中，需要将这两种方法结合使用。</p>
<h6 id="2-7-4-gossip-协议的通讯方式"><a href="#2-7-4-gossip-协议的通讯方式" class="headerlink" title="2.7.4 gossip 协议的通讯方式"></a>2.7.4 gossip 协议的通讯方式</h6><p>不管是 Anti-Entropy 还是 Rumor-Mongering 都涉及到节点间的数据交互方式，节点间的交互方式主要有三种：Push、Pull 以及 Push&amp;Pull。</p>
<p>Push：发起信息交换的节点 A 随机选择联系节点 B，并向其发送自己的信息，节点 B 在收到信息后更新比自己新的数据，一般拥有新信息的节点才会作为发起节点。<br>Pull：发起信息交换的节点 A 随机选择联系节点 B，并从对方获取信息。一般无新信息的节点才会作为发起节点。<br>Push&amp;Pull：发起信息交换的节点 A 向选择的节点 B 发送信息，同时从对方获取数据，用于更新自己的本地数据。</p>
<h2 id="Chapter-II-作业描述与实现"><a href="#Chapter-II-作业描述与实现" class="headerlink" title="Chapter II. 作业描述与实现"></a>Chapter II. 作业描述与实现</h2><h4 id="1-作业描述"><a href="#1-作业描述" class="headerlink" title="1.作业描述"></a>1.作业描述</h4><h5 id="1-1-简述"><a href="#1-1-简述" class="headerlink" title="1.1 简述"></a>1.1 简述</h5><p>使用Gossiping协议实现去中心化的平均数算法（使用任意语言均可）</p>
<blockquote>
<p>假设初始所有GossipNode线程的Message为0，为Passive即不包含任何消息的节点，而在第一轮迭代开始时一个GossipNode的Messge为10，为Active，则该Node将会随机选择其他节点进行信息传播即感染，而进行选择的Node与被选择的Node会消除两者之间的信息差异，即进行Message取平均数，被感染的Node与初始Message为10的Node的Message都变为5，此后获得了信息的Node即为Active可以在每轮循环中随机选择其他（<strong>包括已经被隔离的</strong>)节点进行感染，直到他兴趣值不断降低被确认为DeadNode被Kill也就是隔离为止 ，<strong>隔离</strong>仅仅意味着该节点无法主动选取其他节点进行信息传播</p>
</blockquote>
<h5 id="1-2-定义"><a href="#1-2-定义" class="headerlink" title="1.2 定义"></a>1.2 定义</h5><table>
<thead>
<tr>
<th align="left">Definition</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left">误差</td>
<td>整个算法的核心在于携带初始信息的节点在Gossip之后使得其他节点通过传播均取得相同的信息，假设 InitialActiveNode.getMessage() = 10, 而numberOfNodes = 10,则初始信息的平均值与理想状况下Gossip结束后的Message平均值应该都为1。在实际算法运行中，可能最后节点携带的信息值为1.05或0.98一类的，此时用 理想的平均值 1 减去 其信息值 , 求差值之后将所有节点的差值求平均值，即为所求算法误差的平均值</td>
</tr>
<tr>
<td align="left">收敛轮数</td>
<td>整个Gossip从开始到终止迭代的轮数</td>
</tr>
<tr>
<td align="left">K 值</td>
<td>K是用来计算兴趣值的影响因子</td>
</tr>
<tr>
<td align="left">节点隔离</td>
<td><strong>隔离</strong>仅仅意味着该节点无法主动选取其他节点进行信息传播，被隔离的节点仍然可以被其他ActiveNode选择为信息传播的目标。具体实现方法为 在每轮gossip开始之前，所有独立的ActiveNode生成一个随机数verdictNum(裁决数字)，取值范围在（0,1）之内,对每个Active的节点进行判定，若该节点的valueOfInterest &lt; verdictNum，则该节点被隔离 (一次生成一个verdictNum对所有节点进行相同条件的判定，sounds fair, 但是并不适用于分布式系统)</td>
</tr>
<tr>
<td align="left">兴趣值</td>
<td>获得了信息的Node即为Active可以在每轮循环中随机选择其他节点进行感染，在每轮GossipNode开始感染之前，其成功传播的概率为valueOfInterest，每轮开始传播之前进行一次判定，也可以认为valueOfInterest为该节点存活的概率，若判定无法传播也即没有兴趣，会被确立为死节点并被清除, valueOfInterest *= 1/k  ,k为常值,他选择的有可能为Message为0的PassiveNode也有可能为Message不为0的其他ActiveNode,而当一个Message = c 的 ActiveNode与另一个同样Message = c的Node进行交换时，由于信息相同而受挫，每次受挫都会导致该Node的兴趣值降低，即为</td>
</tr>
</tbody></table>
<p>$$<br>valueOfInterest = 1/(k)^n   【n为受挫的次数】<br>$$</p>
<p>可以理解为 valueOfInterest 为每轮Gossip开始时决定该节点是否继续存活保持Active而不被隔离的几率</p>
<h5 id="1-3-结果要求"><a href="#1-3-结果要求" class="headerlink" title="1.3 结果要求"></a>1.3 结果要求</h5><table>
<thead>
<tr>
<th>Requirements ( 难度 =  Level 5 )</th>
</tr>
</thead>
<tbody><tr>
<td>1.误差与K值之间的关系图</td>
</tr>
<tr>
<td>2.误差与节点个数之间的关系图</td>
</tr>
<tr>
<td>3.收敛轮数与K值之间的关系图</td>
</tr>
<tr>
<td>4.收敛轮数与节点个数之间的关系图</td>
</tr>
<tr>
<td>5.多线程模拟多节点代码</td>
</tr>
<tr>
<td>6.输入输出结果展示</td>
</tr>
</tbody></table>
<h4 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2.代码实现"></a>2.代码实现</h4><h5 id="1-GossipNode"><a href="#1-GossipNode" class="headerlink" title="1.GossipNode"></a>1.GossipNode</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.company;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Objects;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A class representing a gossip Node.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vellichor</span></span><br><span class="line"><span class="comment"> * ID: 20175045</span></span><br><span class="line"><span class="comment"> * Name: 高歌</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GossipNode</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * message ---- 信息</span></span><br><span class="line"><span class="comment">     * downTimes ---------------受挫次数</span></span><br><span class="line"><span class="comment">     * valueOfInterest ------- 兴趣值</span></span><br><span class="line"><span class="comment">     * rounds-----------轮数</span></span><br><span class="line"><span class="comment">     * status ---------- 1 - Active      0- Passive      2- Dead</span></span><br><span class="line"><span class="comment">     * id ---------- 节点 ID 在生成 ArrayList时特定设立为与 Thread 线程与 节点索引相同的数字便于识别</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> Double message;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> downTimes;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> Double valueOfInterest;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rounds;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> status;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    GossipNode(Double message, <span class="keyword">int</span> downTimes, Double valueOfInterest, <span class="keyword">int</span> rounds, <span class="keyword">int</span> status, <span class="keyword">int</span> id) &#123;</span><br><span class="line">        <span class="keyword">this</span>.message = message;</span><br><span class="line">        <span class="keyword">this</span>.downTimes = downTimes;</span><br><span class="line">        <span class="keyword">this</span>.valueOfInterest = valueOfInterest;</span><br><span class="line">        <span class="keyword">this</span>.rounds = rounds;</span><br><span class="line">        <span class="keyword">this</span>.status = status;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * return a random number which is not equal to another unwanted number</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> unwantedNum</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> numberRange</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getRandomNum</span><span class="params">(<span class="keyword">int</span> unwantedNum, <span class="keyword">int</span> numberRange)</span> </span>&#123;</span><br><span class="line">        Random random = <span class="keyword">new</span> Random();</span><br><span class="line">        <span class="keyword">int</span> randomNum = random.nextInt(numberRange) + <span class="number">1</span>;<span class="comment">//用于生成该节点在ArrayList中的索引</span></span><br><span class="line">        <span class="keyword">if</span> (randomNum == unwantedNum) &#123;</span><br><span class="line">            getRandomNum(unwantedNum, numberRange);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> randomNum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 随机生成并返回一个[0.1)的 Double数值 用于判断节点是否被隔离</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Double <span class="title">generatorDouble0To1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Random().nextDouble();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 用于求 两个节点交换信息后的平均信息值</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> Message_1</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> Message_2</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Double <span class="title">getAvgMessage</span><span class="params">(Double Message_1, Double Message_2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (Message_1 + Message_2) / <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> targetNode      当前节点所选择感染的目标节点</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value_K         K值  用于影响兴趣值的递减</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> minusToleration 对于判断节点携带信息是否相同时所用的误差限度（因为 Double 为高精度数字）</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 1--节点继续保持活跃  2----节点被隔离</span></span><br><span class="line"><span class="comment">     * （默认 status = 0 时节点为未携带信息时的 Passive 状态）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">startGossiping</span><span class="params">(GossipNode targetNode, <span class="keyword">int</span> value_K, Double minusToleration)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//每轮遍历ArrayList找寻存活的Active节点进行一轮的感染</span></span><br><span class="line">        System.out.println();</span><br><span class="line">        System.out.println(<span class="string">"当前节点  Message = "</span> + message + <span class="string">" down"</span> +</span><br><span class="line">                <span class="string">"Times =  "</span> + downTimes + <span class="string">"兴趣值valueOfInterest = "</span> + valueOfInterest + <span class="string">"状态Status = "</span> + status);</span><br><span class="line">        System.out.println(<span class="string">"生成（0，1）内的随机数 对该节点进行判定"</span>);</span><br><span class="line">        Double verdictNum = generatorDouble0To1();</span><br><span class="line">        System.out.println(<span class="string">"生成随机数为 "</span> + verdictNum);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (valueOfInterest &gt;= verdictNum) &#123;</span><br><span class="line">            System.out.println(<span class="string">"兴趣值上界 &gt;= 判定数字  在兴趣值区间内，节点继续保持活跃"</span>);</span><br><span class="line">            <span class="comment">//若节点为活跃状态，开始寻找被感染者</span></span><br><span class="line">            <span class="comment">//轮数加1</span></span><br><span class="line">            rounds++;</span><br><span class="line">            System.out.println(<span class="string">"&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 轮数:  "</span> + rounds + <span class="string">" &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"</span>);</span><br><span class="line">            System.out.println(<span class="string">"------------------节点 "</span> + id + <span class="string">" 号开始寻找被感染者---------------- "</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//若二者信息相同(由于为高精度Double，故考虑二者绝对值差值在0.05之内就为相同， ActiveNode节点受挫， 受挫次数 downTimes+1</span></span><br><span class="line">            <span class="keyword">double</span> minusValue = targetNode.message - message;</span><br><span class="line">            <span class="keyword">if</span> (Math.abs(minusValue) &lt; minusToleration) &#123;</span><br><span class="line">                downTimes++;</span><br><span class="line">                <span class="comment">//根据改变的受挫次数设置 新的兴趣值</span></span><br><span class="line">                valueOfInterest = message / value_K;</span><br><span class="line"></span><br><span class="line">                System.out.println(<span class="string">"二者携带信息相同（信息差值在允许范围内）： currentNodeMessage: "</span> + message + <span class="string">" targetNodeMessage = "</span> + targetNode.message );</span><br><span class="line">                System.out.println(<span class="string">"感染者受挫，"</span> + <span class="string">"downTimes = "</span> + downTimes + <span class="string">"兴趣值valueOfInterest = "</span> + valueOfInterest);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//二者信息 均被设置为 其信息的平均值</span></span><br><span class="line">                Double avgMessage = getAvgMessage(targetNode.message, message);</span><br><span class="line">                targetNode.setMessage(avgMessage);</span><br><span class="line">                message = avgMessage;</span><br><span class="line">                System.out.println(<span class="string">"交换信息后 二者信息均变为 avgMessage = "</span> + avgMessage);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">switch</span>(targetNode.status) &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                    targetNode.setStatus(<span class="number">1</span>);<span class="comment">//该节点状态被设置为活跃</span></span><br><span class="line">                    System.out.println(<span class="string">"节点被感染 Status设置为 1 = Active"</span>);</span><br><span class="line">                    targetNode.setRounds(rounds);<span class="comment">//被感染节点初次被感染，继承当前节点的轮数</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">                    System.out.println(<span class="string">"节点之前已经被感染 Status为 1 = Active 保持不变"</span>);</span><br><span class="line">                <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">                    System.out.println(<span class="string">"被感染者已经被隔离, status 不变 仍然为 2 = Dead "</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;<span class="comment">//节点继续保持活跃</span></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"兴趣值上界 &lt; 判定数字    判定数字不在兴趣值区间内，节点被隔离, 轮数不会增加"</span>);</span><br><span class="line">            targetNode.setStatus(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;<span class="comment">//节点被隔离</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 部分代码省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="2-initGossip"><a href="#2-initGossip" class="headerlink" title="2.initGossip"></a>2.initGossip</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.company;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Semaphore;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">initGossip</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> num;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">initGossip</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.num = num;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化节点列表</span></span><br><span class="line"><span class="comment">     * message ---- 信息  Default----0.0</span></span><br><span class="line"><span class="comment">     * downTimes ---------------受挫次数----Default------0</span></span><br><span class="line"><span class="comment">     * valueOfInterest ------- 兴趣值--Default----1.0</span></span><br><span class="line"><span class="comment">     * rounds-----------轮数--Default------0</span></span><br><span class="line"><span class="comment">     * status ---------- 1 - Active      0- Passive      2- Dead----Default------0</span></span><br><span class="line"><span class="comment">     * id ---------- 节点 ID 在生成 ArrayList时特定设立为与 Thread 线程与 节点索引相同的数字便于识别</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> ArrayList&lt;GossipNode&gt; <span class="title">initGossipNodeList</span><span class="params">(<span class="keyword">int</span> num)</span></span>&#123;</span><br><span class="line">        ArrayList&lt;GossipNode&gt; nodeArrayList = <span class="keyword">new</span> ArrayList&lt;&gt;(num);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num; i++) &#123;</span><br><span class="line">            <span class="comment">// 首先将所有节点设置为 Default 条件</span></span><br><span class="line">            GossipNode defaultNode = <span class="keyword">new</span> GossipNode(<span class="number">0.0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>, i);</span><br><span class="line">            nodeArrayList.add(defaultNode);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将第一个节点设置为 Status=1--Active &amp; Message=1.0</span></span><br><span class="line">        nodeArrayList.get(<span class="number">0</span>).setStatus(<span class="number">1</span>);</span><br><span class="line">        nodeArrayList.get(<span class="number">0</span>).setMessage(<span class="number">1.0</span>);</span><br><span class="line">        System.out.println(<span class="string">"节点初始化全部完成，索引与ID皆为0的 节点被设置为 Message = 1.0 的初始感染节点"</span>);</span><br><span class="line">        <span class="keyword">return</span> nodeArrayList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化信号量都为1个</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> ArrayList&lt;Semaphore&gt; <span class="title">initSemaphoreList</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Semaphore&gt; semaphoreArrayList = <span class="keyword">new</span> ArrayList&lt;&gt;(num);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num; i++) &#123;</span><br><span class="line">            semaphoreArrayList.add(<span class="keyword">new</span> Semaphore(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">"信号量初始化完成"</span>);</span><br><span class="line">        <span class="keyword">return</span> semaphoreArrayList;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-GossipingMultiThread"><a href="#3-GossipingMultiThread" class="headerlink" title="3.GossipingMultiThread"></a>3.GossipingMultiThread</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.company;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.text.DecimalFormat;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Semaphore;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GossipingMultiThread</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * value_K : K值 决定兴趣值递减程度</span></span><br><span class="line"><span class="comment">     * numOfNodes : 节点数  同时也为线程数</span></span><br><span class="line"><span class="comment">     * minusToleration : 判断 Message 是否相同时的误差容忍度</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> GossipSettings  settings = <span class="keyword">new</span> GossipSettings(<span class="number">3</span>,<span class="number">20</span>,<span class="number">0.01</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> numOfNodes = settings.getNumberOfnodes();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> value_K = settings.getValue_K();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Double minusToleration = settings.getMinusToleration();<span class="comment">//判断Message是否相同时候的误差容忍度</span></span><br><span class="line">    <span class="comment">//调用 initGossip中的方法，在建立ArrayList时已经完成了 ArrayList中数据的初始化</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ArrayList&lt;GossipNode&gt; nodeArrayList = initGossip.initGossipNodeList(numOfNodes);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ArrayList&lt;Semaphore&gt; semaphoreArrayList = initGossip.initSemaphoreList(numOfNodes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用ArrayList控制线程池中线程的关闭</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ArrayList&lt;Integer&gt; threadPoolStartList = <span class="keyword">new</span> ArrayList&lt;Integer&gt;(numOfNodes);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ArrayList&lt;Integer&gt; threadPoolEndList = <span class="keyword">new</span> ArrayList&lt;Integer&gt;(numOfNodes);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将控制台Console中的输出全部打印到 consoleOutput.txt 中</span></span><br><span class="line">        PrintStream oldPrintStream = System.out;</span><br><span class="line">        FileOutputStream bos = <span class="keyword">new</span> FileOutputStream(<span class="string">"consoleOutput.txt"</span>);</span><br><span class="line">        MultiOutputStream multi = <span class="keyword">new</span> MultiOutputStream(<span class="keyword">new</span> PrintStream(bos),oldPrintStream);</span><br><span class="line">        System.setOut(<span class="keyword">new</span> PrintStream(multi));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;numOfNodes;i++)&#123;</span><br><span class="line">            threadPoolStartList.add(<span class="number">0</span>);</span><br><span class="line">            threadPoolEndList.add(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//用于判断线程池的</span></span><br><span class="line">        <span class="comment">// 开线程池，每个线程搭载一个节点</span></span><br><span class="line">        ExecutorService ex = Executors.newCachedThreadPool();</span><br><span class="line">        ex.execute(<span class="keyword">new</span> ActiveMessageThread(<span class="number">0</span>));</span><br><span class="line">        <span class="comment">// 关闭线程池的条件</span></span><br><span class="line">        <span class="keyword">int</span> allEqual = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//        while (shutDownSign == 0 )&#123;</span></span><br><span class="line">        <span class="keyword">while</span> (allEqual == <span class="number">0</span> &amp;&amp; !threadPoolEndList.isEmpty() )&#123;</span><br><span class="line">            allEqual = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;numOfNodes;i++)&#123;</span><br><span class="line">                <span class="keyword">if</span> (threadPoolStartList.get(i).equals(threadPoolEndList.get(i)))&#123;</span><br><span class="line">                    allEqual = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    allEqual = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//如果记录线程开关的两个 ArrayList 对应索引位置的数值都相同 前 第一个启动的线程已经结束</span></span><br><span class="line">            <span class="comment">//终止线程池</span></span><br><span class="line"><span class="comment">//            if (threadPoolEndList.get(0) == 1 &amp;&amp; allEqual ==1)&#123;</span></span><br><span class="line"><span class="comment">//                shutDownSign = 1;</span></span><br><span class="line"><span class="comment">//                System.out.println("如果记录线程开关的两个 ArrayList 对应索引位置的数值都相同 而且 第一个启动的线程已经结束 ");</span></span><br><span class="line"><span class="comment">//                System.out.println("终止线程池");</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        ex.shutdown();</span><br><span class="line">        <span class="comment">// 确认线程池运行完毕</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        //控制台输出可能受线程池关闭时间影响不准,准确输出以该文件为准</span></span><br><span class="line"><span class="comment">//        writeResult();</span></span><br><span class="line">        <span class="comment">// 写日志</span></span><br><span class="line">        CsvWriter.writeResultToCSV(nodeArrayList,value_K);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;numOfNodes;i++)&#123;</span><br><span class="line">            System.out.println(nodeArrayList.get(i).getRounds());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 线程类</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ActiveMessageThread</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> currentIndex;<span class="comment">// 该线程搭载的信息发送节点编号</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="title">ActiveMessageThread</span><span class="params">(<span class="keyword">int</span> currentIndex)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.currentIndex = currentIndex;<span class="comment">// 初始化时为线程指定所搭载的节点编号</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">                <span class="comment">//序号为currentIndex的线程开始运行</span></span><br><span class="line">                <span class="comment">//threadPoolStartList中索引为currentIndex的数值设为1</span></span><br><span class="line">                threadPoolStartList.set(currentIndex,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> gossipResult = <span class="number">1</span>;<span class="comment">// 初始化是否传递成功</span></span><br><span class="line">                <span class="keyword">int</span> minResource = <span class="number">0</span>;<span class="comment">// 初始化较小编号资源的编号</span></span><br><span class="line">                <span class="keyword">int</span> maxResource = <span class="number">0</span>;<span class="comment">// 初始化较大编号资源的编号</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 随机选择要传达的节点,不能是自己</span></span><br><span class="line">                    <span class="keyword">int</span> targetIndex;</span><br><span class="line">                    <span class="keyword">do</span> &#123;</span><br><span class="line">                        targetIndex = (<span class="keyword">int</span>) (Math.random() * numOfNodes);</span><br><span class="line">                    &#125; <span class="keyword">while</span> (targetIndex == currentIndex);</span><br><span class="line">                    <span class="comment">// 进行资源排序，防止死锁</span></span><br><span class="line">                    <span class="keyword">if</span> (currentIndex &gt; targetIndex) &#123;</span><br><span class="line">                        maxResource = currentIndex;</span><br><span class="line">                        minResource = targetIndex;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        maxResource = targetIndex;</span><br><span class="line">                        minResource = currentIndex;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 先获取小编号信号量</span></span><br><span class="line">                    semaphoreArrayList.get(minResource).acquire();</span><br><span class="line">                    <span class="comment">// 才能获取大编号信号量</span></span><br><span class="line">                    semaphoreArrayList.get(maxResource).acquire();</span><br><span class="line">                    <span class="keyword">int</span> targetStatus = nodeArrayList.get(targetIndex).getStatus();</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 通过startGossiping发送消息并且获取 当前节点状态</span></span><br><span class="line">                    <span class="comment">//1---------节点继续保持活跃</span></span><br><span class="line">                    <span class="comment">//2----------节点被隔离</span></span><br><span class="line">                    <span class="comment">// gossipResult 表示这轮感染成功开始  targetStatus == 0 表示目标节点尚未被感染 可以开启新线程</span></span><br><span class="line">                    gossipResult = nodeArrayList.get(currentIndex).startGossiping(nodeArrayList.get(targetIndex), value_K, minusToleration);</span><br><span class="line">                    <span class="keyword">if</span> (gossipResult == <span class="number">1</span> &amp;&amp; targetStatus == <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="comment">// 如果感染了新的节点,就启动搭载他的线程,</span></span><br><span class="line">                        <span class="comment">// 这里如果感染者和易感者的数本来就相等,线程也会启动,因为相当于感染者把感染全部节点的需求传达给了易感者</span></span><br><span class="line">                        <span class="keyword">new</span> Thread(<span class="keyword">new</span> ActiveMessageThread(targetIndex)).start();</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    <span class="comment">// 集体释放信号量</span></span><br><span class="line">                    semaphoreArrayList.get(minResource).release();</span><br><span class="line">                    semaphoreArrayList.get(maxResource).release();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// gossipResult = 2 代表节点已经被隔离</span></span><br><span class="line">                <span class="keyword">if</span> (gossipResult == <span class="number">2</span>) &#123;<span class="comment">// 终止线程</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment">//序号为currentIndex的线程开始运行</span></span><br><span class="line">                    <span class="comment">//threadPoolEndList中索引为currentIndex的数值设为1</span></span><br><span class="line"></span><br><span class="line">                    threadPoolEndList.set(currentIndex,<span class="number">1</span>);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-输入输出结果展示"><a href="#3-输入输出结果展示" class="headerlink" title="3.输入输出结果展示"></a>3.输入输出结果展示</h4><h6 id="3-1-Console控制台输出展示"><a href="#3-1-Console控制台输出展示" class="headerlink" title="3.1 Console控制台输出展示"></a>3.1 Console控制台输出展示</h6><p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1571926475885.png" alt="1571926475885"></p>
<h6 id="3-2-结果数据打印"><a href="#3-2-结果数据打印" class="headerlink" title="3.2 结果数据打印"></a>3.2 结果数据打印</h6><p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1571926503527.png" alt="1571926503527"></p>
<h4 id="4-结果分析"><a href="#4-结果分析" class="headerlink" title="4.结果分析"></a>4.结果分析</h4><p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1571924466380.png" alt="1571924466380"></p>
<p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1571924632284.png" alt="1571924632284"></p>
<p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1571924730193.png" alt="1571924730193"></p>
<p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1571924850345.png" alt="1571924850345"></p>
<p><img src="./Gossip%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0(multi-thread).assets/1571925811832.png" alt="1571925811832"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Vellichor</p>
              <p class="site-description motion-element" itemprop="description">Mainly record the thoughts & techniques in shool learning & rearch in AI</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vellichor</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/2.1.3/jquery.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
