<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="How to train your Deep Neural NetworkJan 5, 2017 There are certain practices in Deep Learning that are highly recommended, in order to efficiently train Deep Neural Networks. In this post, I will be c">
<meta property="og:type" content="article">
<meta property="og:title" content="How to train your Deep Neural Network">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;11&#x2F;21&#x2F;How%20to%20train%20your%20Deep%20Neural%20Network&#x2F;index.html">
<meta property="og:site_name" content="Vellichor Kyst">
<meta property="og:description" content="How to train your Deep Neural NetworkJan 5, 2017 There are certain practices in Deep Learning that are highly recommended, in order to efficiently train Deep Neural Networks. In this post, I will be c">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-12-09T06:51:04.000Z">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/11/21/How to train your Deep Neural Network/"/>





  <title>How to train your Deep Neural Network | Vellichor Kyst</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Vellichor Kyst</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"> </p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/21/How%20to%20train%20your%20Deep%20Neural%20Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vellichor">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vellichor Kyst">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">How to train your Deep Neural Network</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-21T18:01:57+08:00">
                2019-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="How-to-train-your-Deep-Neural-Network"><a href="#How-to-train-your-Deep-Neural-Network" class="headerlink" title="How to train your Deep Neural Network"></a>How to train your Deep Neural Network</h1><p>Jan 5, 2017</p>
<p>There are certain practices in <strong>Deep Learning</strong> that are highly recommended, in order to efficiently train <strong>Deep Neural Networks</strong>. In this post, I will be covering a few of these most commonly used practices, ranging from importance of quality training data, choice of hyperparameters to more general tips for faster prototyping of DNNs. Most of these practices, are validated by the research in academia and industry and are presented with mathematical and experimental proofs in research papers like <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">Efficient BackProp(Yann LeCun et al.)</a> and <a href="https://arxiv.org/pdf/1206.5533v2.pdf" target="_blank" rel="noopener">Practical Recommendations for Deep Architectures(Yoshua Bengio)</a>.</p>
<p>As you’ll notice, I haven’t mentioned any mathematical proofs in this post. All the points suggested here, should be taken more of a summarization of the best practices for training DNNs. For more in-depth understanding, I highly recommend you to go through the above mentioned research papers and references provided at the end.</p>
<hr>
<h3 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h3><p>A lot of ML practitioners are habitual of throwing raw training data in any <strong>Deep Neural Net(DNN)</strong>. And why not, any DNN would(presumably) still give good results, right? But, it’s not completely old school to say that - “given the right type of data, a fairly simple model will provide better and faster results than a complex DNN”(although, this might have exceptions). So, whether you are working with <strong>Computer Vision</strong>, <strong>Natural Language Processing</strong>, <strong>Statistical Modelling</strong>, etc. try to preprocess your raw data. A few measures one can take to get better training data:</p>
<ul>
<li>Get your hands on as large a dataset as possible(DNNs are quite data-hungry: more is better)</li>
<li>Remove any training sample with corrupted data(short texts, highly distorted images, spurious output labels, features with lots of null values, etc.)</li>
<li>Data Augmentation - create new examples(in case of images - rescale, add noise, etc.)</li>
</ul>
<h3 id="Choose-appropriate-activation-functions"><a href="#Choose-appropriate-activation-functions" class="headerlink" title="Choose appropriate activation functions"></a>Choose appropriate activation functions</h3><p>One of the vital components of any Neural Net are <a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="noopener">activation functions</a>. <strong>Activations</strong> introduces the much desired <strong>non-linearity</strong> into the model. For years, <code>sigmoid</code> activation functions have been the preferable choice. But, a <code>sigmoid</code> function is inherently cursed by these two drawbacks - 1. Saturation of sigmoids at tails(further causing <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" target="_blank" rel="noopener">vanishing gradient problem</a>). 2. <code>sigmoids</code> are not zero-centered.</p>
<p>A better alternative is a <code>tanh</code> function - mathematically, <code>tanh</code> is just a rescaled and shifted <code>sigmoid</code>, <code>tanh(x) = 2*sigmoid(x) - 1</code>. Although <code>tanh</code> can still suffer from the <strong>vanishing gradient problem</strong>, but the good news is - <code>tanh</code> is zero-centered. Hence, using <code>tanh</code> as activation function will result into faster convergence. I have found that using <code>tanh</code> as activations generally works better than sigmoid.</p>
<p>You can further explore other alternatives like <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener"><code>ReLU</code></a>, <code>SoftSign</code>, etc. depending on the specific task, which have shown to ameliorate some of these issues.</p>
<h3 id="Number-of-Hidden-Units-and-Layers"><a href="#Number-of-Hidden-Units-and-Layers" class="headerlink" title="Number of Hidden Units and Layers"></a>Number of Hidden Units and Layers</h3><p>Keeping a larger number of hidden units than the optimal number, is generally a safe bet. Since, any regularization method will take care of superfluous units, at least to some extent. On the other hand, while keeping smaller numbers of hidden units(than the optimal number), there are higher chances of underfitting the model.</p>
<p>Also, while employing <strong>unsupervised pre-trained representations</strong>(describe in later sections), the optimal number of hidden units are generally kept even larger. Since, pre-trained representation might contain a lot of irrelevant information in these representations(for the specific supervised task). By increasing the number of hidden units, model will have the required flexibility to filter out the most appropriate information out of these pre-trained representations.</p>
<p>Selecting the optimal number of layers is relatively straight forward. As <a href="https://www.quora.com/profile/Yoshua-Bengio" target="_blank" rel="noopener">@Yoshua-Bengio</a> mentioned on Quora - “You just keep on adding layers, until the test error doesn’t improve anymore”. ;)</p>
<h3 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h3><p>Always initialize the weights with small <code>random numbers</code> to break the symmetry between different units. But how small should weights be? What’s the recommended upper limit? What probability distribution to use for generating random numbers? Furthermore, while using <code>sigmoid</code> activation functions, if weights are initialized to very large numbers, then the sigmoid will <strong>saturate</strong>(tail regions), resulting into <strong>dead neurons</strong>. If weights are very small, then gradients will also be small. Therefore, it’s preferable to choose weights in an intermediate range, such that these are distributed evenly around a mean value.</p>
<p>Thankfully, there has been lot of research regarding the appropriate values of initial weights, which is really important for an efficient convergence. To initialize the weights that are evenly distributed, a <code>uniform distribution</code> is probably one of the best choice. Furthermore, as shown in the <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">paper(Glorot and Bengio, 2010)</a>, units with more incoming connections(fan_in) should have relatively smaller weights.</p>
<p>Thanks to all these thorough experiments, now we have a tested formula that we can directly use for weight initialization; i.e. - weights drawn from <code>~ Uniform(-r, r)</code> where <code>r=sqrt(6/(fan_in+fan_out))</code> for <code>tanh</code> activations, and <code>r=4*(sqrt(6/fan_in+fan_out))</code> for <code>sigmoid</code> activations, where <code>fan_in</code> is the size of the previous layer and <code>fan_out</code> is the size of next layer.</p>
<h3 id="Learning-Rates"><a href="#Learning-Rates" class="headerlink" title="Learning Rates"></a>Learning Rates</h3><p>This is probably one of the most important hyperparameter, governing the learning process. Set the learning rate too small and your model might take ages to converge, make it too large and within initial few training examples, your loss might shoot up to sky. Generally, a learning rate of <code>0.01</code> is a safe bet, but this shouldn’t be taken as a stringent rule; since the optimal learning rate should be in accordance to the specific task.</p>
<p>In contrast to, a fixed learning rate, gradually decreasing the learning rate, after each epoch or after a few thousand examples is another option. Although this might help in faster training, but requires another manual decision about the new learning rates. Generally, <strong>learning rate can be halved after each epoch</strong> - these kinds of strategies were quite common a few years back.</p>
<p>Fortunately, now we have better <code>momentum based methods</code> to change the learning rate, based on the curvature of the error function. It might also help to set different learning rates for individual parameters in the model; since, some parameters might be learning at a relatively slower or faster rate.</p>
<p>Lately, there has been a good amount of research on optimization methods, resulting into <code>adaptive learning rates</code>. At this moment, we have numerous options starting from good old <code>Momentum Method</code> to <code>Adagrad</code>, <code>Adam</code>(personal favourite ;)), <code>RMSProp</code> etc. Methods like <code>Adagrad</code> or <code>Adam</code>, effectively save us from manually choosing an <code>initial learning rate</code>, and given the right amount of time, the model will start to converge quite smoothly(of course, still selecting a good initial rate will further help).</p>
<h3 id="Hyperparameter-Tuning-Shun-Grid-Search-Embrace-Random-Search"><a href="#Hyperparameter-Tuning-Shun-Grid-Search-Embrace-Random-Search" class="headerlink" title="Hyperparameter Tuning: Shun Grid Search - Embrace Random Search"></a>Hyperparameter Tuning: Shun Grid Search - Embrace Random Search</h3><p><strong>Grid Search</strong> has been prevalent in classical machine learning. But, Grid Search is not at all efficient in finding optimal hyperparameters for DNNs. Primarily, because of the time taken by a DNN in trying out different hyperparameter combinations. As the number of hyperparameters keeps on increasing, computation required for Grid Search also increases exponentially.</p>
<p>There are two ways to go about it:</p>
<ol>
<li>Based on your prior experience, you can manually tune some common hyperparameters like learning rate, number of layers, etc.</li>
<li>Instead of Grid Search, use <strong>Random Search/Random Sampling</strong> for choosing optimal hyperparameters. A combination of hyperparameters is generally choosen from a <strong>uniform distribution</strong> within the desired range. It is also possible to add some prior knowledge to further decrease the search space(like learning rate shouldn’t be too large or too small). Random Search has been found to be way more efficient compared to Grid Search.</li>
</ol>
<h3 id="Learning-Methods"><a href="#Learning-Methods" class="headerlink" title="Learning Methods"></a>Learning Methods</h3><p>Good old <strong>Stochastic Gradient Descent</strong> might not be as efficient for DNNs(again, not a stringent rule), lately there have been a lot of research to develop more flexible optimization algorithms. For e.g.: <code>Adagrad</code>, <code>Adam</code>, <code>AdaDelta</code>, <code>RMSProp</code>, etc. In addition to providing <strong>adaptive learning rates</strong>, these sophisticated methods also use <strong>different rates for different model parameters</strong> and this generally results into a smoother convergence. It’s good to consider these as hyper-parameters and one should always try out a few of these on a subset of training data.</p>
<h3 id="Keep-dimensions-of-weights-in-the-exponential-power-of-2"><a href="#Keep-dimensions-of-weights-in-the-exponential-power-of-2" class="headerlink" title="Keep dimensions of weights in the exponential power of 2"></a>Keep dimensions of weights in the exponential power of 2</h3><p>Even, when dealing with <strong>state-of-the-art</strong> Deep Learning Models with latest hardware resources, <strong>memory management</strong> is still done at the byte level; So, it’s always good to keep the size of your parameters as <code>64</code>, <code>128</code>, <code>512</code>, <code>1024</code>(all powers of <code>2</code>). This might help in sharding the matrices, weights, etc. resulting into slight boost in learning efficiency. This becomes even more significant when dealing with <strong>GPUs</strong>.</p>
<h3 id="Unsupervised-Pretraining"><a href="#Unsupervised-Pretraining" class="headerlink" title="Unsupervised Pretraining"></a>Unsupervised Pretraining</h3><p>Doesn’t matter whether you are working with NLP, Computer Vision, Speech Recognition, etc. <strong>Unsupervised Pretraining</strong> always help the training of your supervised or other unsupervised models. <strong>Word Vectors</strong> in NLP are ubiquitous; you can use <a href="http://image-net.org/" target="_blank" rel="noopener">ImageNet</a> dataset to pretrain your model in an unsupervised manner, for a 2-class supervised classification; or audio samples from a much larger domain to further use that information for a speaker disambiguation model.</p>
<h3 id="Mini-Batch-vs-Stochastic-Learning"><a href="#Mini-Batch-vs-Stochastic-Learning" class="headerlink" title="Mini-Batch vs. Stochastic Learning"></a>Mini-Batch vs. Stochastic Learning</h3><p>Major objective of training a model is to learn appropriate parameters, that results into an optimal mapping from inputs to outputs. These parameters are tuned with each training sample, irrespective of your decision to use <strong>batch</strong>, <strong>mini-batch</strong> or <strong>stochastic learning</strong>. While employing a stochastic learning approach, gradients of weights are tuned after each training sample, introducing noise into gradients(hence the word ‘stochastic’). This has a very desirable effect; i.e. - with the introduction of <strong>noise</strong> during the training, the model becomes less prone to overfitting.</p>
<p>However, going through the stochastic learning approach might be relatively less efficient; since now a days machines have far more computation power. Stochastic learning might effectively waste a large portion of this. If we are capable of computing <strong>Matrix-Matrix multiplication</strong>, then why should we limit ourselves, to iterate through the multiplications of individual pairs of <strong>Vectors</strong>? Therefore, for greater throughput/faster learning, it’s recommended to use mini-batches instead of stochastic learning.</p>
<p>But, selecting an appropriate batch size is equally important; so that we can still retain some noise(by not using a huge batch) and simultaneously use the computation power of machines more effectively. Commonly, a batch of <code>16</code> to <code>128</code> examples is a good choice(exponential of <code>2</code>). Usually, batch size is selected, once you have already found more important hyperparameters(by <strong>manual search</strong> or <strong>random search</strong>). Nevertheless, there are scenarios when the model is getting the training data as a stream(<a href="https://en.wikipedia.org/wiki/Online_machine_learning" target="_blank" rel="noopener">online learning</a>), then resorting to Stochastic Learning is a good option.</p>
<h3 id="Shuffling-training-examples"><a href="#Shuffling-training-examples" class="headerlink" title="Shuffling training examples"></a>Shuffling training examples</h3><p>This comes from <strong>Information Theory</strong> - “Learning that an unlikely event has occurred is more informative than learning that a likely event has occurred”. Similarly, randomizing the order of training examples(in different epochs, or mini-batches) will result in faster convergence. A slight boost is always noticed when the model doesn’t see a lot of examples in the same order.</p>
<h3 id="Dropout-for-Regularization"><a href="#Dropout-for-Regularization" class="headerlink" title="Dropout for Regularization"></a>Dropout for Regularization</h3><p>Considering, millions of parameters to be learned, regularization becomes an imperative requisite to prevent <strong>overfitting</strong> in DNNs. You can keep on using <strong>L1/L2</strong> regularization as well, but <strong>Dropout</strong> is preferable to check overfitting in DNNs. Dropout is trivial to implement and generally results into faster learning. A default value of <code>0.5</code> is a good choice, although this depends on the specific task,. If the model is less complex, then a dropout of <code>0.2</code> might also suffice.</p>
<p>Dropout should be turned off, during the test phase, and weights should be scaled accordingly, as done in the <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener">original paper</a>. Just allow a model with Dropout regularization, a little bit more training time; and the error will surely go down.</p>
<h3 id="Number-of-Epochs-Training-Iterations"><a href="#Number-of-Epochs-Training-Iterations" class="headerlink" title="Number of Epochs/Training Iterations"></a>Number of Epochs/Training Iterations</h3><p>“Training a Deep Learning Model for multiple epochs will result in a better model” - we have heard it a couple of times, but how do we quantify “many”? Turns out, there is a simple strategy for this - Just keep on training your model for a fixed amount of examples/epochs, let’s say <code>20,000</code> examples or <code>1</code> epoch. After each set of these examples compare the <strong>test error</strong> with <strong>train error</strong>, if the gap is decreasing, then keep on training. In addition to this, after each such set, save a copy of your model parameters(so that you can choose from multiple models once it is trained).</p>
<h3 id="Visualize"><a href="#Visualize" class="headerlink" title="Visualize"></a>Visualize</h3><p>There are a thousand ways in which the training of a deep learning model might go wrong. I guess we have all been there, when the model is being trained for hours or days and only after the training is finished, we realize something went wrong. In order to save yourself from bouts of hysteria, in such situations(which might be quite justified ;)) - <strong>always visualize the training process</strong>. Most obvious step you can take is to <strong>print/save logs</strong> of <code>loss</code> values, <code>train error</code> or <code>test error</code>, etc.</p>
<p>In addition to this, another good practice is to use a visualization library to plot histograms of weights after few training examples or between epochs. This might help in keeping track of some of the common problems in Deep Learning Models like <strong>Vanishing Gradient</strong>, <strong>Exploding Gradient</strong> etc.</p>
<h3 id="Multi-Core-machines-GPUs"><a href="#Multi-Core-machines-GPUs" class="headerlink" title="Multi-Core machines, GPUs"></a>Multi-Core machines, GPUs</h3><p>Advent of GPUs, libraries that provide vectorized operations, machines with more computation power, are probably some of the most significant factors in the success of Deep Learning. If you think, you are patient as a stone, you might try running a DNN on your laptop(which can’t even open 10 tabs in your Chrome browser) and wait for ages to get your results. Or you can play smart(and expensively :z) and get a descent hardware with at least <strong>multiple CPU cores</strong> and a <strong>few hundred GPU cores</strong>. GPUs have revolutionized the Deep Learning research(no wonder Nvidia’s stocks are shooting up ;)), primarily because of their ability to perform Matrix Operations at a larger scale.</p>
<p>So, instead of taking weeks on a normal machine, these parallelization techniques, will bring down the training time to days, if not hours.</p>
<h3 id="Use-libraries-with-GPU-and-Automatic-Differentiation-Support"><a href="#Use-libraries-with-GPU-and-Automatic-Differentiation-Support" class="headerlink" title="Use libraries with GPU and Automatic Differentiation Support"></a>Use libraries with GPU and Automatic Differentiation Support</h3><p>Thankfully, for rapid prototyping we have some really descent libraries like <a href="http://deeplearning.net/software/theano/" target="_blank" rel="noopener">Theano</a>, <a href="https://www.tensorflow.org/" target="_blank" rel="noopener">Tensorflow</a>, <a href="https://keras.io/" target="_blank" rel="noopener">Keras</a>, etc. Almost all of these DL libraries provide <strong>support for GPU computation</strong> and <strong>Automatic Differentiation</strong>. So, you don’t have to dive into core GPU programming(unless you want to - it’s definitely fun :)); nor you have to write your own differentiation code, which might get a little bit taxing in really complex models(although you should be able to do that, if required). Tensorflow further provides support for training your models on a <strong>distributed architecture</strong>(if you can afford it).</p>
<p>This is not at all an exhaustive list of practices, to train a DNN. In order to include just the most common practices, I have tried to exclude a few concepts like Normalization of inputs, Batch/Layer Normalization, Gradient Check, etc. Although feel free to add anything in the comment section and I’ll be more than happy to update it in the post. :)</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/11/12/Git%E7%9A%84%E4%BD%BF%E7%94%A8/" rel="next" title="Git的使用">
                <i class="fa fa-chevron-left"></i> Git的使用
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/17/End-to-End%20Machine%20Learning%20with%20TensorFlow%20on%20GCP/" rel="prev" title="End-to-End Machine Learning with TensorFlow on GCP">
                End-to-End Machine Learning with TensorFlow on GCP <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Vellichor</p>
              <p class="site-description motion-element" itemprop="description">Mainly record the thoughts & techniques in shool learning & rearch in AI</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#How-to-train-your-Deep-Neural-Network"><span class="nav-number">1.</span> <span class="nav-text">How to train your Deep Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-data"><span class="nav-number">1.0.1.</span> <span class="nav-text">Training data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choose-appropriate-activation-functions"><span class="nav-number">1.0.2.</span> <span class="nav-text">Choose appropriate activation functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Number-of-Hidden-Units-and-Layers"><span class="nav-number">1.0.3.</span> <span class="nav-text">Number of Hidden Units and Layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">1.0.4.</span> <span class="nav-text">Weight Initialization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Rates"><span class="nav-number">1.0.5.</span> <span class="nav-text">Learning Rates</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hyperparameter-Tuning-Shun-Grid-Search-Embrace-Random-Search"><span class="nav-number">1.0.6.</span> <span class="nav-text">Hyperparameter Tuning: Shun Grid Search - Embrace Random Search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Methods"><span class="nav-number">1.0.7.</span> <span class="nav-text">Learning Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Keep-dimensions-of-weights-in-the-exponential-power-of-2"><span class="nav-number">1.0.8.</span> <span class="nav-text">Keep dimensions of weights in the exponential power of 2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised-Pretraining"><span class="nav-number">1.0.9.</span> <span class="nav-text">Unsupervised Pretraining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mini-Batch-vs-Stochastic-Learning"><span class="nav-number">1.0.10.</span> <span class="nav-text">Mini-Batch vs. Stochastic Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffling-training-examples"><span class="nav-number">1.0.11.</span> <span class="nav-text">Shuffling training examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-for-Regularization"><span class="nav-number">1.0.12.</span> <span class="nav-text">Dropout for Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Number-of-Epochs-Training-Iterations"><span class="nav-number">1.0.13.</span> <span class="nav-text">Number of Epochs/Training Iterations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Visualize"><span class="nav-number">1.0.14.</span> <span class="nav-text">Visualize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Core-machines-GPUs"><span class="nav-number">1.0.15.</span> <span class="nav-text">Multi-Core machines, GPUs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Use-libraries-with-GPU-and-Automatic-Differentiation-Support"><span class="nav-number">1.0.16.</span> <span class="nav-text">Use libraries with GPU and Automatic Differentiation Support</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vellichor</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/2.1.3/jquery.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
